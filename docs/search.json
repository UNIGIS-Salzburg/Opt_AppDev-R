[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Automated Data Processing with R",
    "section": "",
    "text": "Preface\nThis web-book is a text book with exercises that together form the learning materials for “Automated Data Processing with R”, an elective module of the UNIGIS distance learning program in Geoinformatics at the University of Salzburg.\nThe web-book is published under an open licence. We welcome everybody to explore the contents and to work through the exercises. A certificate of completion and online support by a tutor can exclusively be claimed by those who are signed up for this UNIGIS module at the University of Salzburg.\n\nFor more information, please get in contact with the UNIGIS Office.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01-introduction.html",
    "href": "01-introduction.html",
    "title": "1  Introduction to R",
    "section": "",
    "text": "1.1 About this module\nThis module will equip you with fundamental R programming skills, beginning with core programming concepts common to most languages. These include Datatypes, Operators, Variables, Functions, Control Structures, and Libraries. You will gain a solid understanding of these basics, forming a strong foundation for advanced programming.\nWe then progress to more complex data types like Data Frames and Tibbles, and explore how to Read and Write both spatial and non-spatial datasets. Special emphasis is placed on techniques to manipulate data, enabling you to adeptly manage and analyze datasets. This will be particularly useful for preparing and refining data for in-depth analysis.\nThe module also focuses on data visualization, where you’ll learn to create informative and compelling visual representations, such as box plots, scatterplots, line plots, and maps. These skills are critical for data exploration and presenting your findings in an accessible manner.\nAdditionally, the course introduces the essentials of working with spatial data. This includes handling spatial data structures, performing spatial data manipulation, and understanding spatial relationships.\nUpon completing this module, you’ll possess foundational R programming skills, preparing you for more advanced topics such as “Geospatial Data Analysis”, as covered in the “Spatial Statistics” module of the MSc program.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-introduction.html#about-this-module",
    "href": "01-introduction.html#about-this-module",
    "title": "1  Introduction to R",
    "section": "",
    "text": "This module partly draws from granolarr, developed by Stefano de Sabbata at University of Leicester. For further exploration, refer to the Webbook R for Geographic Data Science. We particularly recommend its chapters on Statistical Analysis and Machine Learning for those interested in advanced R applications.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-introduction.html#r-programming-language",
    "href": "01-introduction.html#r-programming-language",
    "title": "1  Introduction to R",
    "section": "\n1.2 R programming language",
    "text": "1.2 R programming language\nR is versatile in data science and analytics, with applications including:\n\nData wrangling\nStatistical analysis\nMachine learning\nData visualization and mapping\nSpatial data processing\nGeographic information analysis\n\nWhy R stands out:\n\nFree and open-source\nOffers extensive functionality surpassing most proprietary tools\nAvailable across Windows, MacOS, and Linux\nPrimarily a domain-specific language with a focus on statistics and data analysis, it’s also versatile enough for general-purpose programming, making it ideal for automating analyses and creating custom functions.\nLarge, supportive community, facilitating problem-solving and knowledge sharing\n\nR, a high-level programming or scripting language, relies on an interpreter instead of a compiler. This interpreter directly executes written instructions, requiring adherence to the programming language’s grammar or Syntax.\nIn this lesson we will focus on some key principles of the R syntax and logic.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-introduction.html#installation-and-setup",
    "href": "01-introduction.html#installation-and-setup",
    "title": "1  Introduction to R",
    "section": "\n1.3 Installation and Setup",
    "text": "1.3 Installation and Setup\nBefore you can run your code, you have to install R together with an Integrated Development Environment (IDE) on your machine:\n\nDownload R from R Archive Network (CRAN.)\n\nInstall the latest version, choosing ‘base’ and the appropriate bit-version for your OS.\n\nThe IDE is where you write, test, and execute your R programs, we strongly recommend using RStudio Desktop, which is freely available for download.\nThis video offers a concise RStudio overview Figure 1.1:\n\n\n\n\n\n\nFigure 1.1: Video (6:09 min): RStudio for the Total Beginner.\n\n\n\n\n\n\n\n\nTip\n\n\n\nEncounter technical difficulties? Please consult the discussion forum!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-introduction.html#interpreting-values",
    "href": "01-introduction.html#interpreting-values",
    "title": "1  Introduction to R",
    "section": "\n1.4 Interpreting Values",
    "text": "1.4 Interpreting Values\nWith RStudio and R installed, let’s dive into coding. The Console Window in Figure 1.2 is where the interpreter outputs results based on your input.\n\n\n\n\n\nFigure 1.2: Console Window in RStudio\n\n\nType in a numeric value (e.g., 3) and press Enter. The interpreter returns the input value preceded by a bracketed number. The value in brackets indicates that the input is composed of one single entity.\n\n\n\n\n\n\nExercise\n\n\n\nWhat happens when you input a text value (e.g., ‘test’)?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe interpreter returns an error when unquoted text is entered, as it’s not recognized as a string. In R, text or strings must be enclosed in quotes (either single 'test' or double \"test\") to be understood as character data. Text is commonly referred to as String or String of Characters.\n\n\n\nIf you start your input with a hash symbol (#) the interpreter will consider that line as a comment. For instance, if you type in # Test Test Test, you will see that nothing is returned as an output. Comments are extremely important as they allow you to add explanations in plain language. Comments are fundamental to allow other people to understand your code and it will save you time interpreting your own code.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-introduction.html#simple-data-types",
    "href": "01-introduction.html#simple-data-types",
    "title": "1  Introduction to R",
    "section": "\n1.5 Simple Data Types",
    "text": "1.5 Simple Data Types\nR’s simple data types, essential for encoding information, include:\n\nnumeric\n\nboth integer and real numbers\n\n\n\ncharacter\n\ni.e., “text”, also called strings\n\n\n\nlogical\n\nRepresents TRUE or FALSE values\n\n\n\nLogical TRUE or FALSE values are typically the result of evaluating logical expressions.\nTogether these three simple data types are the building blocks R uses to encode information.\n\n\n\n\n\n\nExercise\n\n\n\nIf you type a simple numeric operation in the console (e.g. 2 + 4), the interpreter will return a result. This indicates that operations (e.g. mathematical calculations) can be carried out on these types.\nLogical operations return values of type logical. What value is returned in the console when you type and execute the expression 2 &lt; 3?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe interpreter returns TRUE, because it is true that 2 is less than 3.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-introduction.html#numop",
    "href": "01-introduction.html#numop",
    "title": "1  Introduction to R",
    "section": "\n1.6 Numeric operators",
    "text": "1.6 Numeric operators\nR provides a series of basic numeric operators:\n\n\nOperator\nMeaning\nExample\nOutput\n\n\n\n+\nPlus\n5 + 2\n7\n\n\n-\nMinus\n5 - 2\n3\n\n\n*\nProduct\n5 * 2\n10\n\n\n/\nDivision\n5 / 2\n2.5\n\n\n%/%\nInteger division\n5 %/% 2\n2\n\n\n%%\nModulo\n5 %% 2\n1\n\n\n^\nPower\n5^2\n25\n\n\n\nWhereas mathematical operators are self-explanatory, the operators Modulo and Integer division may be new to some of you. Integer division returns an integer quotient:\n\n5%/%2\n\n[1] 2\n\n\n\nNote: The code above returns a value of 2. The number in squared brackets [1] indicates the line number of the return.\n\n\n\n\n\n\n\nExercise\n\n\n\nExecute 5 %% 2 to test the ‘Modulo’ operator.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe “Modulo” returns the remainder of the division, which is 1 in the example above.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-introduction.html#logical-operators",
    "href": "01-introduction.html#logical-operators",
    "title": "1  Introduction to R",
    "section": "\n1.7 Logical operators",
    "text": "1.7 Logical operators\nR also provides a series of basic logical operators to create logical expressions:\n\n\nOperator\nMeaning\nExample\nOutput\n\n\n\n==\nEquality\n5 == 2\nFALSE\n\n\n!=\nInequality\n5 != 2\nTRUE\n\n\n&gt; (&gt;=)\nGreater (or equal)\n5 &gt; 2\nTRUE\n\n\n&lt; (&lt;=)\nLess (or equal)\n5 &lt;= 2\nFALSE\n\n\n!\nNegation\n!TRUE\nFALSE\n\n\n&\nLogical AND\nTRUE & FALSE\nFALSE\n\n\n|\nLogical OR\nTRUE | FALSE\nTRUE\n\n\n\nLogical expressions are typically used to execute code dependent on the occurrence of conditions.\n\n\n\n\n\n\nExercise\n\n\n\nWhat logical values are returned by the following expressions:\n\n(3 != 5) | (3 == 4)\n(2 &gt;= 3) | (3 &lt; 7)\n(2 == 9) & (2 &lt; 4)\n\nType and execute these expressions in the RStudio console to validate your assumptions.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-introduction.html#references",
    "href": "01-introduction.html#references",
    "title": "1  Introduction to R",
    "section": "\n1.8 References",
    "text": "1.8 References\nApart from Stefano de Sabbata’s teaching materials, this module draws from various sources, most of which are available online:\n\n\nThe Grammar Of Graphics – All You Need to Know About ggplot2 and Pokemons by Pascal Schmidt. See Online Tutorial\n\n\nggplot2 - Overview. See Online Documentation\n\n\nGetting started with httr2 - httr2 quickstart guide. See Online Tutorial\n\n\nProgramming Skills for Data Science: Start Writing Code to Wrangle, Analyze, and Visualize Data with R by Michael Freeman and Joel Ross, Addison-Wesley, 2019. See Book Webpage and Repository.\n\nR for Data Science (2e) by Garrett Grolemund and Hadley Wickham, O’Reilly Media, 2016. See Online Book.\n\nMachine Learning with R: Expert techniques for predictive modeling by Brett Lantz, Packt Publishing, 2019. See Book Webpage.\n\nIntroduction to spatial data in R by Nils Riach and Rafael Hologa. See Online Tutorial.\n\nThe Art of R Programming: A Tour of Statistical Software Design by Norman Matloff, No Starch Press, 2011. See Book Webpage\n\n\nAn Introduction to R for Spatial Analysis and Mapping by Chris Brunsdon and Lex Comber, Sage, 2015. See Book Webpage\n\n\nGeocomputation with R by Robin Lovelace, Jakub Nowosad, Jannes Muenchow, CRC Press, 2019. See Online Book.\n\nDiscovering Statistics Using R by Andy Field, Jeremy Miles and Zoë Field, SAGE Publications Ltd, 2012. See Book Webpage.\n\nThe RStudio Cheatsheets - A collection of cheatsheets for various R functions and packages. View Collection on RStudio Website.\n\nThe terra package - A comprehensive guide to the ‘terra’ package for spatial analysis in R. View Online Documentation\n\n\nSf - Simple Features for R - Documentation on using the ‘sf’ package for handling spatial data in R. View Online Documentation",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "02-core-concepts.html",
    "href": "02-core-concepts.html",
    "title": "2  Core Concepts",
    "section": "",
    "text": "2.1 Variables\nIn this lesson, we’ll explore three fundamental concepts in programming:\nVariables are essential for storing data. To define a variable, use an identifier (e.g., a_variable) on the left of an assignment operator &lt;-, followed by the value you wish to assign (e.g., 1):\na_variable &lt;- 1\nTo retrieve the value of the variable, simply use its identifier:\na_variable\n\n[1] 1\nVariables allow you to store computational results and later access them for further analysis. For instance:\na_variable &lt;- 1\na_variable &lt;- a_variable + 10\nanother_variable &lt;- a_variable\nWhy use variables instead of direct input? They make your code reusable, scalable, and time-efficient, especially with complex data analyses or larger datasets.\nLet us consider the following example:\nMeteorologists track water temperature gradients to forecast weather patterns. If location A’s temperature is 22°C and B’s is 26°C, calculating the difference as 26 - 22 in the RStudio console is straightforward. However, for ongoing real-time measurements, an algorithm using variables for each location’s temperature can significantly speed up the process. Such an algorithm is adaptable to various data inputs, making it a robust tool for complex calculations.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Core Concepts</span>"
    ]
  },
  {
    "objectID": "02-core-concepts.html#variables",
    "href": "02-core-concepts.html#variables",
    "title": "2  Core Concepts",
    "section": "",
    "text": "Tip\n\n\n\nTo save and manage your code efficiently, create an R Script in RStudio (File &gt; New File &gt; R Script). Select the code in the R Script Window and click ‘Run’ to execute it. Scripts offer the advantage of running multiple lines of code at once and keeping a record of your work.\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nIn RStudio, create a new R script (File &gt; New File &gt; R Script).\nDeclare two variables (temp_A and temp_B) with temperature values.\nDeclare a third variable (diff) to store their difference.\nRun your script and observe the changes in the Environment Panel.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ntemp_A &lt;- 24\ntemp_B &lt;- 28\ndiff &lt;- temp_A - temp_B\n\nAfter executing the code in RStudio, you should notice changes in the Environment Panel, located in the top right corner. The Environment Panel will display three memory slots with identifiers diff, temp_A, and temp_B, having values of -4, 24, and 28, respectively. Invoking the name of an identifier in the code (e.g., typing diff and running it) will return the value stored in that memory slot.\nTo clear your workspace memory, click the “broom icon” in the Environment Panel’s menu.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Core Concepts</span>"
    ]
  },
  {
    "objectID": "02-core-concepts.html#algorithms-and-functions",
    "href": "02-core-concepts.html#algorithms-and-functions",
    "title": "2  Core Concepts",
    "section": "\n2.2 Algorithms and Functions",
    "text": "2.2 Algorithms and Functions\n\n“An algorithm is a mechanical rule, automatic method, or program for performing some mathematical operation” (Cutland, 1980).\n\nA program is a specific set of instructions that implements an abstract algorithm.\nThe definition of an algorithm (and thus a program) can consist of one or more functions. Functions are sets of instructions that perform a task, structuring code into functional, reusable units. Some functions receive values as inputs, while others return output values.\nProgramming languages typically provide pre-defined functions that implement common algorithms (e.g., finding the square root of a number or calculating a linear regression).\nFor example, the pre-defined function sqrt() calculates the square root of an input value. Like all functions in R, sqrt() is invoked by specifying the function name and arguments (input values) between parentheses:\n\nsqrt(2)\n\n[1] 1.414214\n\n\nEach input value corresponds to a parameter defined in the function.\n\n\n\n\n\n\nTip\n\n\n\nIn programming, the terms ‘parameter’ and ‘argument’ are often used interchangeably, but there is a subtle distinction. A parameter is the variable listed inside the parentheses in the function declaration, while an argument is the actual value passed to the function. For example, in the function definition square(number), number is a parameter. When you call square(4), the value 4 is the argument passed to the number parameter. Understanding this difference helps in comprehending how functions receive and process information.\n\n\nround() is another predefined function in R:\n\nround(1.414214, digits = 2)\n\n[1] 1.41\n\n\nNote that the name of the second parameter (digits) needs to be specified. The digits parameter indicates the number of digits to keep after the decimal point.\nThe return value of a function can be stored in a variable:\n\nsqrt_of_two &lt;- sqrt(2)\nsqrt_of_two\n\n[1] 1.414214\n\n\nHere, the output value is stored in a memory slot with the identifier sqrt_of_two. We can use the identifier sqrt_of_two as an argument in other functions:\n\nsqrt_of_two &lt;- sqrt(2)\nround(sqrt_of_two, digits = 3)\n\n[1] 1.414\n\n\nThe first line calculates the square root of 2 and stores it in a variable named sqrt_of_two. The second line rounds the value stored in sqrt_of_two to three decimal places.\n\n\n\n\n\n\nExercise\n\n\n\nCan you store the output of the round() function in a second variable?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsqrt_of_two &lt;- sqrt(2)\nrounded_sqrt_of_two &lt;- round(sqrt_of_two, digits = 3)\n\n\n\n\nFunctions can also be used as arguments within other functions. For instance, we can use sqrt() as the first argument in round():\n\nround(sqrt(2), digits = 3)\n\n[1] 1.414\n\n\nIn this case, the intermediate step of storing the square root of 2 in a variable is skipped.\nWhile using functions as arguments within other functions (nested functions) can sometimes reduce readability, they are a common and powerful practice in R. This approach can make your code more concise and expressive. However, it’s essential to balance conciseness with clarity!\nMoreover, to enhance the readability of R code, it is recommended to follow naming conventions for variables and functions:\n\nR is a case-sensitive language, meaning UPPER and lower case are treated as distinct.\n\n\na_variable is not the same as a_VARIABLE.\n\n\nValid names can include:\n\nAlphanumeric characters, . and _.\n\n\nNames must start with a letter, not a number or a symbol.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Core Concepts</span>"
    ]
  },
  {
    "objectID": "02-core-concepts.html#libraries",
    "href": "02-core-concepts.html#libraries",
    "title": "2  Core Concepts",
    "section": "\n2.3 Libraries",
    "text": "2.3 Libraries\nRelated, reusable functions in R can be grouped and stored in libraries, also known as packages.\nAs of now, there are more than 10,000 R libraries available. These can be downloaded and installed using the install.packages() function. Once a library is installed, the library() function is used to make it accessible in a script.\nLibraries can vary greatly in size and complexity. For example:\n\n\nbase: This includes basic R functions, such as the sqrt() function discussed earlier.\n\nsf: A package providing simple feature access.\n\nThe stringr library illustrates the use of libraries in R. It offers a consistent and well-defined set of functions for string manipulation. Assuming the library is already installed on your computer, you can load it as follows:\n\nlibrary(stringr)\n\nIf not yet installed, you can download and install it with:\n\ninstall.packages('stringr')  # Note: the function takes a string argument ('')\n\n\n\n\n\n\n\nTip\n\n\n\nAlternatively, libraries (a.k.a. packages) can be installed through RStudio’s ‘Install Packages’ menu (Tools &gt; Install Packages…). You can choose to install from CRAN or a package archive file.\n\n\nThe majority of libraries are available through CRAN - Comprehensive R Archive Network, a vast collection of R resources and libraries.\n\nWhen using install.packages(), the package name must be a string, hence the quotes around ‘stringr’.\nTo explore the functions provided by a library, like stringr, you can use ls() to list them. This command shows you what’s available to use once a library is loaded into your R session.\n\nOnce installed and loaded, the library provides a new set of functions in your environment. For instance, str_length in the stringr library returns the number of characters in a string:\n\nstr_length(\"UNIGIS\")\n\n[1] 6\n\n\nThe str_detect() function returns TRUE if the first argument (a string) contains the second argument (a character string). Otherwise, it returns FALSE:\n\nstr_detect(\"UNIGIS\", \"I\")\n\n[1] TRUE\n\n\nstr_replace_all replaces all instances of a specified character in a string with another character:\n\nstr_replace_all(\"UNIGIS\", \"I\", 'X')\n\n[1] \"UNXGXS\"\n\n\nYou can list all the functions available in the stringr library using the built-in function ls():\n\nls(\"package:stringr\")\n\n [1] \"%&gt;%\"               \"boundary\"          \"coll\"             \n [4] \"fixed\"             \"fruit\"             \"invert_match\"     \n [7] \"regex\"             \"sentences\"         \"str_c\"            \n[10] \"str_conv\"          \"str_count\"         \"str_detect\"       \n[13] \"str_dup\"           \"str_ends\"          \"str_equal\"        \n[16] \"str_escape\"        \"str_extract\"       \"str_extract_all\"  \n[19] \"str_flatten\"       \"str_flatten_comma\" \"str_glue\"         \n[22] \"str_glue_data\"     \"str_interp\"        \"str_length\"       \n[25] \"str_like\"          \"str_locate\"        \"str_locate_all\"   \n[28] \"str_match\"         \"str_match_all\"     \"str_order\"        \n[31] \"str_pad\"           \"str_rank\"          \"str_remove\"       \n[34] \"str_remove_all\"    \"str_replace\"       \"str_replace_all\"  \n[37] \"str_replace_na\"    \"str_sort\"          \"str_split\"        \n[40] \"str_split_1\"       \"str_split_fixed\"   \"str_split_i\"      \n[43] \"str_squish\"        \"str_starts\"        \"str_sub\"          \n[46] \"str_sub_all\"       \"str_sub&lt;-\"         \"str_subset\"       \n[49] \"str_to_lower\"      \"str_to_sentence\"   \"str_to_title\"     \n[52] \"str_to_upper\"      \"str_trim\"          \"str_trunc\"        \n[55] \"str_unique\"        \"str_view\"          \"str_view_all\"     \n[58] \"str_which\"         \"str_width\"         \"str_wrap\"         \n[61] \"word\"              \"words\"",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Core Concepts</span>"
    ]
  },
  {
    "objectID": "03-data-structures.html",
    "href": "03-data-structures.html",
    "title": "3  Data Structures",
    "section": "",
    "text": "3.1 Vectors\nIn this lesson, we expand upon the simple data types (numeric, character and logical) discussed in Lesson 1 by introducing more complex data structures.\nIn this lesson, you will get to know the following data structures in R:\nA Vector is an ordered list of values. Vectors can be of any of the following simple types:\nHowever, all items in a vector must be of the same type. A vector can be of any length.\nDefining a vector variable is similar to declaring a simple type variable, but the vector is created using the function c(), which combines values into a vector:\n# Declare a vector variable of strings\na_vector &lt;- c(\"Birmingham\", \"Derby\", \"Leicester\", \"Lincoln\", \"Nottingham\", \"Wolverhampton\")\na_vector\n\n[1] \"Birmingham\"    \"Derby\"         \"Leicester\"     \"Lincoln\"      \n[5] \"Nottingham\"    \"Wolverhampton\"\nNote that the second line of the returned elements starts with [5], as it begins with the fifth element of the vector.\nOther functions for creating vectors include seq() and rep():\n# Create a vector of real numbers with an interval of 0.5 between 1 and 7\na_vector &lt;- seq(1, 7, by = 0.5)\na_vector\n\n [1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0\n# Create a vector with four identical character string values\na_vector &lt;- rep(\"Ciao\", 4)\na_vector\n\n[1] \"Ciao\" \"Ciao\" \"Ciao\" \"Ciao\"\nNumeric vectors can also be created using a simple syntax:\n# Create a vector of integer numbers from 1 to 10\na_vector &lt;- 1:10\na_vector\n\n [1]  1  2  3  4  5  6  7  8  9 10",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Structures</span>"
    ]
  },
  {
    "objectID": "03-data-structures.html#vectors",
    "href": "03-data-structures.html#vectors",
    "title": "3  Data Structures",
    "section": "",
    "text": "Numeric\nCharacter\nLogical\n\n\n\n\n\n\n\n\n\n\n\n3.1.1 Vector Element Selection\nYou can access individual elements of a vector by specifying the index of the element between square brackets, following the vector’s identifier. Remember, in R, the first element of a vector has an index of 1. For example, to retrieve the third element of a vector named a_vector:\n\na_vector &lt;- 3:8\na_vector[3]  # Retrieves the third element\n\n[1] 5\n\n\nTo retrieve multiple elements, use a vector of indices:\n\na_vector &lt;- 3:8\na_vector[c(2, 4)]  # Retrieves the second and fourth elements\n\n[1] 4 6\n\n\nIn this case, the values 4 and 6 are returned, corresponding to indices 2 and 4 in a_vector.\n\nNote that the vector of indices (c(2, 4)) is created on the fly without declaring a variable name.\n\n\n\n\n\n\n\nExercise\n\n\n\nTry creating and selecting elements from a vector yourself. Follow these steps:\n\nCreate a vector named east_midlands_cities containing the cities: Derby, Leicester, Lincoln, Nottingham.\nSelect the last three cities and assign them to a new vector named selected_cities.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\neast_midlands_cities &lt;- c(\"Derby\", \"Leicester\", \"Lincoln\", \"Nottingham\")\n\nmy_indexes &lt;- 2:4\n\nselected_cities &lt;- east_midlands_cities[my_indexes]\n\n\n\n\n\n3.1.2 Using the range() Function with Vectors\nThe range() function in R, is used to find the minimum and maximum values within a vector. This can be particularly helpful when analyzing the spread of data in a vector. For example:\n\n# Create a numeric vector\nnumeric_vector &lt;- c(2, 8, 4, 16, 6)\n\n# Apply the range() function\nvector_range &lt;- range(numeric_vector)\nvector_range  # Displays the minimum and maximum values\n\n[1]  2 16\n\n\n\n3.1.3 Applying Functions to Vectors\nIn R, functions can be applied to vectors just like they are with individual variables. When a function is applied to a vector, it typically processes each element of the vector, resulting in a new vector of the same length as the input.\nFor instance, adding a value (like 10) to a numeric vector will add that value to each element of the vector:\n\nnumeric_vector &lt;- 1:5\nnumeric_vector &lt;- numeric_vector + 10  # Adds 10 to each element\nnumeric_vector\n\n[1] 11 12 13 14 15\n\n\nSimilarly, applying a function like sqrt() to a numeric vector will compute the square root of each element:\n\nnumeric_vector &lt;- 1:5\nnumeric_vector &lt;- sqrt(numeric_vector)\nnumeric_vector  # Displays the square roots\n\n[1] 1.000000 1.414214 1.732051 2.000000 2.236068\n\n\nA logical condition applied to a vector will return a logical vector indicating whether each element meets the condition:\n\nnumeric_vector &lt;- 1:5\nlogical_vector &lt;- numeric_vector &gt;= 3\nlogical_vector  # Shows TRUE or FALSE for each element\n\n[1] FALSE FALSE  TRUE  TRUE  TRUE\n\n\nMoreover, functions like any() and all() provide overall evaluations of a vector based on a condition. any() returns TRUE if any elements satisfy the condition, while all() returns TRUE only if all elements satisfy the condition:\n\nnumeric_vector &lt;- 1:5\nany(numeric_vector &gt;= 3)  # Checks if any element is &gt;= 3\n\n[1] TRUE\n\nall(numeric_vector &gt;= 3)  # Checks if all elements are &gt;= 3\n\n[1] FALSE\n\n\nAlso, when creating vectors in R, it’s important to understand the concept of type coercion. R is designed to be user-friendly, and when you combine different data types in a vector (e.g., mixing numbers and characters), R will automatically convert all elements to the same type. This process is known as type coercion. For example, if you combine numeric and character data in a vector, all elements will become characters.\n\nmixed_vector &lt;- c(1, \"text\", TRUE)\nprint(mixed_vector)  # Notice how all elements are coerced to the same type\n\n[1] \"1\"    \"text\" \"TRUE\"\n\n\n\n\n\n\n\n\nTip\n\n\n\nFactors are a special data type in R, similar to vectors but limited to predefined values called levels. Factors are not covered in this module, but you can learn more about them in the Programming with R tutorial.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Structures</span>"
    ]
  },
  {
    "objectID": "03-data-structures.html#multi-dimensional-data-types",
    "href": "03-data-structures.html#multi-dimensional-data-types",
    "title": "3  Data Structures",
    "section": "\n3.2 Multi-dimensional Data Types",
    "text": "3.2 Multi-dimensional Data Types\n\n3.2.1 Matrices\nMatrices in R are two-dimensional data structures, where data is organized in rows and columns. They are particularly useful for performing a variety of mathematical operations.\nTo create a matrix, use the matrix() function, providing a vector of values and the desired dimensions:\n\na_matrix &lt;- matrix(c(3, 5, 7, 4, 3, 1), nrow=3, ncol=2)\na_matrix\n\n     [,1] [,2]\n[1,]    3    4\n[2,]    5    3\n[3,]    7    1\n\n\nR supports numerous operators and functions for matrix algebra. For example, basic arithmetic operations can be performed on matrices:\n\nx &lt;- matrix(c(3, 5, 7, 4, 3, 1), nrow=3, ncol=2)\ny &lt;- matrix(c(1, 2, 3, 4, 5, 6), nrow=3, ncol=2)\nz &lt;- x * y  # Element-wise multiplication\nz\n\n     [,1] [,2]\n[1,]    3   16\n[2,]   10   15\n[3,]   21    6\n\n\nWhen working with matrices, range() can help you quickly identify the lowest and highest values within a particular row or column. However, it’s not typically used for selecting rows or columns. Instead, you’d use direct indexing or other functions for selection. Here’s how to correctly utilize range() with matrices::\n\n# Creating a matrix with numeric values\nmatrix_data &lt;- matrix(1:9, nrow=3)\n\n# Finding the range of values in the first column\nfirst_column_range &lt;- range(matrix_data[,1])\nprint(first_column_range)  # Displays the minimum and maximum values of the first column\n\n[1] 1 3\n\n\nIn the context of matrix selection, while range() is not used for selecting specific rows or columns, understanding the spread of data within a matrix can be crucial for informed data manipulation and analysis. Here’s an example of how you might use this information:\n\n# Assuming you want to know if the first column contains values within a specific range\nis_in_range &lt;- first_column_range[1] &gt;= 2 && first_column_range[2] &lt;= 8\nprint(is_in_range)  # Checks if the range of the first column is between 2 and 8\n\n[1] FALSE\n\n\nOr you can exclude specific columns or rows from a matrix using negative indexing. This is particularly useful for analysis or visualization when you want to focus on specific parts of the matrix:\n\n# Creating a matrix\nmatrix_data &lt;- matrix(1:9, nrow=3)\n\n# Excluding the first column from the matrix\nmatrix_without_first_column &lt;- matrix_data[, -1]  # Excludes the first column\nprint(matrix_without_first_column)\n\n     [,1] [,2]\n[1,]    4    7\n[2,]    5    8\n[3,]    6    9\n\n\nFor a detailed overview of matrix operations, refer to Quick-R.\n\n3.2.2 Arrays\nArrays in R are like higher-dimensional matrices, capable of storing data in multiple dimensions. Creating an array requires specifying the values and the dimensions for each axis:\n\na3dim_array &lt;- array(1:24, dim=c(4, 3, 2))  # Creates a 3-dimensional array\na3dim_array\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n[3,]    3    7   11\n[4,]    4    8   12\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]   13   17   21\n[2,]   14   18   22\n[3,]   15   19   23\n[4,]   16   20   24\n\n\n\nNote: An array can have a single dimension, resembling a vector. However, arrays have additional attributes like dim and offer different functionalities.\n\n\n3.2.3 Selection in Multi-Dimensional Data Types\nSelecting elements from matrices and arrays in R is similar to vector selection, but requires specifying an index for each dimension.\nFor matrices:\n\n# Example matrix\na_matrix &lt;- matrix(c(3, 5, 7, 4, 3, 1), nrow=3, ncol=2)\na_matrix\n\n     [,1] [,2]\n[1,]    3    4\n[2,]    5    3\n[3,]    7    1\n\n# Selecting the second row, first and second columns\na_matrix[2, c(1, 2)]\n\n[1] 5 3\n\n\nFor arrays with multiple dimensions:\n\n# Example 3-dimensional array\nan_array &lt;- array(1:12, dim=c(3, 2, 2))\nan_array\n\n, , 1\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n, , 2\n\n     [,1] [,2]\n[1,]    7   10\n[2,]    8   11\n[3,]    9   12\n\n# Selecting elements with specific indices\nan_array[2, c(1, 2), 2]\n\n[1]  8 11\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCreate a 3-dimensional array, extract 2 elements to form a vector, and 4 elements to form a matrix.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nArray creation:\n\na3dim_array &lt;- array(1:24, dim=c(4, 3, 2))\n\nExtracting elements:\n\na_vector &lt;- a3dim_array[3, c(1, 2), 2]\na_matrix &lt;- a3dim_array[c(3, 4), c(1, 2), 2]\n\n\n\n\n\n3.2.4 Lists\nLists in R are incredibly versatile and can hold elements of different types, including vectors, matrices, other lists, and even functions. This makes lists a powerful tool for organizing and storing complex, heterogeneous collections of data.\nElements in lists are selected using double square brackets.\nBasic list:\n\nemployee &lt;- list(\"Christian\", 2017)\nemployee\n\n[[1]]\n[1] \"Christian\"\n\n[[2]]\n[1] 2017\n\n# Selecting the first element\nemployee[[1]]\n\n[1] \"Christian\"\n\n\nNamed lists allow selection using the $ symbol:\n\n# Named list\nemployee &lt;- list(employee_name = \"Christian\", start_year = 2017)\nemployee\n\n$employee_name\n[1] \"Christian\"\n\n$start_year\n[1] 2017\n\n# Selecting by name\nemployee$employee_name\n\n[1] \"Christian\"\n\n\n\n3.2.5 Data Frame\nData frames are essential in R for representing tables of data. Each data frame is structured similarly to a named list with each element being a vector of equal length. Below is an example of creating a data frame:\n\nemployees &lt;- data.frame(\n  EmployeeName = c(\"Maria\", \"Pete\", \"Sarah\"),\n  Age = c(47, 34, 32),\n  Role = c(\"Professor\", \"Researcher\", \"Researcher\"))\nemployees\n\n  EmployeeName Age       Role\n1        Maria  47  Professor\n2         Pete  34 Researcher\n3        Sarah  32 Researcher\n\n\nData frames are similar to tables in that each column represents a variable, and each row represents an observation.\n\n\n\n\n\n\nExercise\n\n\n\nCan elements of different types be mixed within a single vector or data frame column?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nVector elements (and by extension, data frame columns) must be of the same type (character, logical, or numeric). For example, EmployeeName contains characters, while Age contains numerics.\n\n\n\nElements in each column of a data frame correspond to a row. The first element in EmployeeName represents the name of the first employee, and similarly for other columns.\n\n\n\n\n\n\nTip\n\n\n\nTo rename columns, use the ‘names()’ function: names(data frame)[column index] = “new name”\n\n\nSelecting data from a data frame is analogous to vector and list selection, but with a focus on the data frame’s two-dimensional structure. You typically need two indices to extract data.\nExample of selecting the first element in the first column:\n\nemployees[1, 1]\n\n[1] \"Maria\"\n\n\nSelecting whole rows:\n\nemployees[1, ]\n\n  EmployeeName Age      Role\n1        Maria  47 Professor\n\n\nSelecting whole columns:\n\nemployees[, 1]\n\n[1] \"Maria\" \"Pete\"  \"Sarah\"\n\n\nColumns can also be selected using dollar signs and column names:\n\nemployees$Age\n\n[1] 47 34 32\n\nemployees$Age[1]  # Selecting the first element in the 'Age' column\n\n[1] 47\n\n\nModifying a data frame:\n\nChanging an element (e.g., updating Pete’s age):\n\n\nemployees$Age[2] &lt;- 33\n\n\nAdding a new column:\n\n\nemployees$Place &lt;- c(\"Salzburg\", \"Salzburg\", \"Salzburg\")\nemployees\n\n  EmployeeName Age       Role    Place\n1        Maria  47  Professor Salzburg\n2         Pete  33 Researcher Salzburg\n3        Sarah  32 Researcher Salzburg\n\n\n\n\n\n\n\n\nExercise\n\n\n\nOur simple data frame includes columns EmployeeName, Age and Role:\n\nemployees &lt;- data.frame(\n  EmployeeName = c(\"Maria\", \"Pete\", \"Sarah\"),\n  Age = c(47, 34, 32),\n  Role = c(\"Professor\", \"Researcher\", \"Researcher\"))\n\nIn this exercise you are asked to include an additional column in the data frame that contains the year of birth of employees. This new column can be derived from column age.\nThis step will most likely require consultation of other online resources.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nCreating a data frame employees:\n\nemployees &lt;- data.frame(\n  EmployeeName = c(\"Maria\", \"Pete\", \"Sarah\"),\n  Age = c(47, 34, 32),\n  Role = c(\"Professor\", \"Researcher\", \"Researcher\"))\n\nCalculating the current_year:\n\ncurrent_year &lt;- as.integer(format(Sys.Date(), \"%Y\"))\n\nCalculating Year_of_birth as extra data frame column:\n\nemployees$Year_of_birth &lt;- current_year - employees$Age\nemployees\n\n  EmployeeName Age       Role Year_of_birth\n1        Maria  47  Professor          1978\n2         Pete  34 Researcher          1991\n3        Sarah  32 Researcher          1993",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Structures</span>"
    ]
  },
  {
    "objectID": "04-spatial-data-structures.html",
    "href": "04-spatial-data-structures.html",
    "title": "4  Spatial Data Structures",
    "section": "",
    "text": "4.1 Vector Data Structures\nIn the realm of geoinformatics, spatial data is a cornerstone, offering a lens through which we can view, analyze, and interpret the world around us. At this point in your studies, you are already familiar with the basic spatial entities of points, lines, and polygons. These fundamental structures, while simple in concept, form the bedrock of complex spatial analyses and visualizations.\nR, with its rich ecosystem of packages, offers a unique perspective on spatial data. Packages like sf, terra, stars, and spatstat have been game-changers, allowing us to handle spatial vector, raster, and multidimensional data with unprecedented ease and flexibility.\nIn this lesson, you will get to know the following spatial data structures in R:\nYou will also learn how to retrieve, assign, and modify coordinate systems, projections and transformations of spatial data structures.\nSpatial data structures are the foundation upon which geospatial information is built. They provide a systematic framework for organizing and representing geographical entities, ensuring that they can be efficiently processed, analyzed, and visualized.\nIn this section, we will use the sf library to work with vector data structures. The name sf (which stands for simple features) implies that sf supports simple feature access via R (sf conforms to the simple feature standard). Simple features is a widely supported data model that underlies data structures in many GIS applications, including QGIS and PostGIS. A major advantage of this is that using the data model ensures your work is cross-transferable to other setups, for example, importing from and exporting to spatial databases.\nSimple features have spatial, geometric attributes as well as non-spatial attributes. The most common geometry types are points, lines, and polygons and their respective “multi” versions (see Simple feature geometry types).\nPoints are fundamental in geospatial analysis. They are the simplest spatial entities, representing a singular location in space. They have no dimensions, meaning they don’t possess length, width, or area. Using the sf package, you can create and manipulate point data with ease:\n# Create a point\npoint &lt;- sf::st_point(c(5, 5))\n\n# Convert to sf object\npoint_sf &lt;- sf::st_sf(geometry = sf::st_sfc(point))\nThe function st_point creates a simple feature object from a numeric vector. The object is of the same nature as the numeric vector c(5,5). To convert to an sf-object, the function st_sf is used.\nSf-objects are similar in structure as data frames. However, in contrast to data frames, sf-objects have an additional geometry column. The sf-object point_sf (created by the code above) contains a single point geometry and no fields:\npoint_sf\n\nSimple feature collection with 1 feature and 0 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 5 ymin: 5 xmax: 5 ymax: 5\nCRS:           NA\n     geometry\n1 POINT (5 5)\nIn principle sf-objects can be treated like data frames. Accordingly, data frame syntax is used to assign fields (table columns) to geometries:\npoint_sf$name &lt;- c(\"my location\")\nTo stay in GIS terms, records (rows) in a sf-object table may be called features.\nFeatures can be composed of multiple geometries:\n# Create three points as multipoint geometry\npoint_multi &lt;- sf::st_multipoint(matrix(c(3, 5, 7, 4, 3, 1), c(3, 2)), dim = \"XY\")\n\n# Convert to sf object\npoint_sf_multi &lt;- sf::st_sf(geometry = sf::st_sfc(point_multi))\n\nplot(point_sf_multi)\nPlotting the metadata of sf-object point_sf_multi in the console reveals that Geometry type is Multipoint and that no coordinate reference system has been defined (CRS: NA):\npoint_sf_multi\n\nSimple feature collection with 1 feature and 0 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: 3 ymin: 1 xmax: 7 ymax: 4\nCRS:           NA\n                        geometry\n1 MULTIPOINT ((3 4), (5 3), (...\nTo extract or set CRS information, use the st_crs function:\n# Assign WGS84 as CRS (EPSG code 4326)\nsf::st_crs(point_sf_multi) &lt;- 4326\nNow we transform the sf-object to EPSG 3416 (Austrian Lambert Projection) with the function st_transform:\n# Assign Austrian Lambert Projection as CRS\npoint_sf_multi_transform &lt;- sf::st_transform(point_sf_multi, 3416)\nEventually we can compare the two sf-objects with different coordinate systems:\n# Plotting the original and transformed points side by side\npar(mfrow = c(1, 2)) # Arrange plots in 1 row, 2 columns\n\n# Plot original points with a coordinate grid and axes\nplot(sf::st_geometry(point_sf_multi), main = \"Original Points (EPSG: 4326)\", pch = 19, col = \"blue\", cex = 1.5)\ngrid()\nbox()\n\n# Plot transformed points\nplot(sf::st_geometry(point_sf_multi_transform), main = \"Transformed Points (EPSG: 3416)\", pch = 19, col = \"red\", cex = 1.5)\ngrid()\nbox()\nThe same syntax and functions can be used to deal with line or polygon data. The following drop-downs contain two simple examples.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Spatial Data Structures</span>"
    ]
  },
  {
    "objectID": "04-spatial-data-structures.html#vector-data-structures",
    "href": "04-spatial-data-structures.html#vector-data-structures",
    "title": "4  Spatial Data Structures",
    "section": "",
    "text": "Tip\n\n\n\nThe operator :: is used to indicate that the functions st_point, st_sf, and st_sfc are situated within the library sf. This helps avoiding ambiguities in the case functions from different loaded libraries have identical names.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nExecute the code above in an R Script. Go to the Environment tab in RStudio and click on point_sf. The sf-object contains a single feature that is composed of a point geometry and the field name.\nAdd one more feature with name value “your location” and arbitrary point geometry value. You may use function rbind (see here) to append features to sf-objects.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Create first feature\npoint &lt;- sf::st_point(c(5, 5))\npoint_sf &lt;- sf::st_sf(geometry = sf::st_sfc(point))\npoint_sf$name &lt;- c(\"my location\")\n\n\n# Create second feature\ngeom = sf::st_sfc(sf::st_point(c(6, 8)))\nn = c(\"your location\")\nsec_point_sf &lt;- sf::st_sf(geometry = geom, name = n)\n\n\n#merge two features into sf-object two_points_sf\ntwo_points_sf &lt;- rbind(point_sf, sec_point_sf)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nEPSG-Codes of other reference systems can be found here.\n\n\n\n\n\n\n\n\nCreate Line Feature!\n\n\n\n\nLines are sequences of points. They’re instrumental in representing pathways, routes, or any linear feature. Here’s how you can create a line using sf:\n\n# Create a line from a matrix of coordinates\nline &lt;- st_linestring(matrix(1:6, 3, 2))\n\n# Convert to spatial feature\nline_sf &lt;- st_sf(geometry = st_sfc(line))\n\n# Plot the line\nplot(line_sf)\n\n\n\n\n\n\n\n\n\n\nCreate Polygon Feature!\n\n\n\n\nPolygons are closed shapes, perfect for representing areas with defined boundaries. Here’s a demonstration using sf and its st_polygon function:\n\n# Create a matrix of coordinates\ncoords &lt;- matrix(c(2,2, 4,4, 4,2, 2,2), ncol = 2, byrow = TRUE)\n\n# Create a list of matrices (in this case, just one matrix)\nlist_of_coords &lt;- list(coords)\n\n# Create the polygon\npolygon &lt;- st_polygon(list_of_coords)\n\n# Convert to spatial feature\npolygon_sf &lt;- st_sf(geometry = st_sfc(polygon))\n\n# Plot the polygon\nplot(polygon_sf)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nThe code above successfully creates and plots a polygon. However, imagine if the last coordinate in the coordinate matrix (2,2) was mistakenly omitted.\nConsider the following questions:\n\nWhat error would you expect to encounter if the last coordinate was omitted?\nWhy is the last coordinate crucial for the creation of the polygon?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIf the last coordinate was omitted, you would encounter the following error:\nError in MtrxSet(x, dim, type = \"POLYGON\", needClosed = TRUE) : polygons not (all) closed\nA breakdown of what the error message is conveying:\n\nMtrxSet(x, dim, type = \"POLYGON\", needClosed = TRUE): This is the internal function being called to set or validate the matrix representation of the polygon.\ntype = \"POLYGON\": This indicates that the data structure being worked on is in fact a Polygon.\nneedClosed = TRUE: This is a condition set within the function to ensure that polygons are closed. It checks if the starting and ending coordinates of the polygon are the same.\npolygons not (all) closed: This is the main error message, indicating that one or more polygons in your data are not closed, i.e., their starting and ending coordinates don’t match.\n\nIn practical terms, if you’re creating or manipulating polygons, you need to ensure that each polygon’s last coordinate is the same as its first coordinate. If not, many spatial operations, analyses, or visualizations might produce incorrect or unexpected results!\nThe last coordinate is crucial because it ensures that the polygon is closed, meaning its starting and ending coordinates are the same.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Spatial Data Structures</span>"
    ]
  },
  {
    "objectID": "04-spatial-data-structures.html#raster-data-structures",
    "href": "04-spatial-data-structures.html#raster-data-structures",
    "title": "4  Spatial Data Structures",
    "section": "\n4.2 Raster Data Structures",
    "text": "4.2 Raster Data Structures\nWhereas man-made infrastructures (streets, buildings, sewage systems etc.) can clearly be delineated and modeled by discrete features, our natural-physical environment (temperature, soil moisture etc.) tends to be continuous by nature and best represented by raster data structures.\nBoth packages sf and terra, can handle raster and vector data. Due to its comprehensive toolset and integration with the tidyverse ecosystem (tidyverse will be covered in lesson Data Manipulation), sf is predominantly used for discrete vector data structures. Until quite recently, the package raster has been the most popular resource to work with continuous data in R. This package is being replaced by terra (see here for more information).\nAccordingly, in this lesson we will focus on the more modern terra package that offers several advantages over its predecessor:\n\nEfficiency: Terra is optimized for speed and uses less memory, making it more efficient for large datasets.\nFlexibility: It supports raster, vector, and time-series data, providing a one-stop solution for various spatial data types.\nEase of Use: With a simplified and consistent syntax, terra is easier to pick up for newcomers.\nComprehensive Functions: From raster algebra to resampling and reclassification, terra offers a wide array of functionalities.\nIntegration: It’s designed to work seamlessly with other R packages, making it easier to integrate into larger workflows.\n\n\n4.2.1 Working with SpatRaster objects\nThe terra SpatRaster Object can be created using the function rast:\n\n# Load the terra package\nlibrary(terra)\n\nx &lt;- terra::rast()\n\nx\n\nclass       : SpatRaster \ndimensions  : 180, 360, 1  (nrow, ncol, nlyr)\nresolution  : 1, 1  (x, y)\nextent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (CRS84) (OGC:CRS84) \n\n\nBy default, the SpatRaster Object is initialized with a global extent and a spatial resolution of 1 degree. The coordinate reference system is WGS84.\nAlternatively, additional arguments may be provided in the function to customize the SpatRaster Object:\n\nx &lt;- terra::rast(ncol=100, nrow=100, xmin=797422, xmax=807387, ymin=5298037, ymax=5306341, crs = \"+proj=utm +zone=32 +ellps=WGS84 +datum=WGS84 +units=m +no_defs \")\n\nx\n\nclass       : SpatRaster \ndimensions  : 100, 100, 1  (nrow, ncol, nlyr)\nresolution  : 99.65, 83.04  (x, y)\nextent      : 797422, 807387, 5298037, 5306341  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=utm +zone=32 +datum=WGS84 +units=m +no_defs \n\n\nIn the example above, arguments such as the number of grid rows and columns (nrow and ncol) as well as the grid extent (xmin, xmax, ymin and ymax) were defined. The raster cell resolution is a result of these inputs (x=99.65, y=83.04).\nAccording to the documentation of the rast function, the coordinate reference system can be specified in PROJ.4, WKT or authority:code notation. In the given example WGS84 UTM 32N is encoded in PROJ.4. To find the desired encoding, it is recommended to first search for a CRS on the Spatial Reference Website. The PROJ.4 is one out of many formats (e.g. EPSG code, WKT, GML etc.) that are provided in the search results.\n\n\n\n\n\n\nExercise\n\n\n\nThe SpatRaster Object x has an extent that covers the City of Salzburg, which is completely within UTM Zone 33N. Search for the PROJ.4 encoding of WGS84 UTM 33N on the Spatial Reference Site.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n+proj=utm +zone=33 +ellps=WGS84 +datum=WGS84 +units=m +no_defs \n\n\n\n\nIn order to change the coordinate reference system from WGS84 UTM 32N to WGS84 UTM 33N and to change the spatial resolution to 100m, the terra-functions project and res can be used:\n\ny &lt;- terra::project(x, \"+proj=utm +zone=33 +ellps=WGS84 +datum=WGS84 +units=m +no_defs\")\nterra::res(y) &lt;- 100\ny\n\nclass       : SpatRaster \ndimensions  : 91, 105, 1  (nrow, ncol, nlyr)\nresolution  : 100, 100  (x, y)\nextent      : 347863.1, 358363.1, 5291599, 5300699  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=utm +zone=33 +datum=WGS84 +units=m +no_defs \n\n\nNote that the number of rows and column was changed to enable a raster resolution of 100m. Also the bounding box coordinates of the SpatRaster Object have changed, which indicates a successful projection of the raster grid from UTM 32 to UTM 33.\n\n\n\n\n\n\nTip\n\n\n\nGiven that the SpatRaster Object has an undefined coordinate reference system, it can be defined by means of terra-function crs.\n\n\nSo far, we have created two SpatRaster Objects (identifiers x and y). They only consist of a skeleton, meaning that they have location, extend, a spatial gird resolution and a certain number of grid rows and columns. However, there are not yet cell-values associated with it:\n\n#check whether objects have values\nterra::hasValues(x)\n\n[1] FALSE\n\nterra::hasValues(y)\n\n[1] FALSE\n\n\nThe function hasValues() returns a logical value FALSE, because no values were assigned to objects x and y. The code below shows how to assign values to empty SpatRaster Object y:\n\n# assign random value between 0 and 1 to cells of SpatRaster Object y\nterra::values(y) &lt;- stats::runif(terra::ncell(y),0,1)\n\n# get values with index 1 to index 5\nterra::values(y)[1:5]\n\n[1] 0.6664615 0.9895084 0.1577042 0.4029686 0.5709577\n\n#plot grid, plot() is a generic function to plot R objects\nplot(y, main='100m Raster, Salzburg')\n\n\n\n\n\n\n\nThe function runif of package stats takes three arguments, n, min and max, to generate n random numbers in a range between min and max. In the example above argument n is derived from terra function ncell, which returns the number of cells of SpatRaster Object y as an integer value. As a result, we get a numeric vector of random values whose length corresponds to the number of grid cells of SpatRaster Object y. Accordingly, we can assign the numeric vector values to the grid.\n\n\n\n\n\n\nExercise\n\n\n\nWhen assigning or accessing values, it is crucial to know the origin and orientation of raster values. Create a SpatRaster Object with 4 cells and assign a numeric vector that consists of 4 values c(1,2,3,4) to it.\nThe syntax to select values from SpatRaster Objects is the same that we have used to select elements of matrix and array data structures:\nraster-obj[&lt;row index&gt;, &lt;column index&gt;]\nWhere is the origin of the grid? In which order are the values c(1,2,3,4) stored in the vector grid?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nr &lt;- terra::rast(ncol=2, nrow=2)\nterra::values(r) &lt;- c(1,2,3,4)\nplot(r)\n\n\n\n\n\n\nr[1,1]\n\n  lyr.1\n1     1\n\n\nr[1,1] returns value 1. Accordingly, the upper left corner is the origin of the SpatRaster Object. Values are stored left to right and top-down, i.e. r[2,2] returns value 4.\n\n\n\nNote that the layer number lyr.1 is returned in the console, when accessing individual values of a SpatRaster Object. This implies that SpatRaster Objects can handle space-time and multivariable data. A useful example for integrating space and time in SpatRaster Objects is provided by Dominic Royé in his blog post Use of multidimensional spatial data. For more sophisticated data cube applications, the use of the stars package is recommended.\nIn this lesson, you learned to handle spatial vector and raster structures in R. To get to know these structures, we built vector and raster objects from scratch. In many instances, however, objects may be created in R by loading vector and raster file formats (e.g. .shp or .tif). This topic will be covered in lesson 8.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Spatial Data Structures</span>"
    ]
  },
  {
    "objectID": "05-control-structures.html",
    "href": "05-control-structures.html",
    "title": "\n5  Control Structures\n",
    "section": "",
    "text": "5.1 If\nIn this lesson, you will learn about control structures, a significant elements in coding that allows for dynamic behavior based on variable values.\nWe distinguish between two types of control structures:\nThe most fundamental conditional statement in R is the structure if, which is used to execute one or more instructions only if a certain condition is TRUE.\nTo include an “if-structure” in your code, you need to use the following syntax:\na_value &lt;- -7\nif (a_value &lt; 0) {\n  cat(\"Negative\")\n}\n\nNegative\nThe statement cat(\"Negative\") is executed and the text “Negative” is printed out, because the condition (a_value &lt; 0) is TRUE.\nThe function cat() concatenates and prints string inputs (“Negative” in the example above). Alternatively, you can use the function print() to write variable values to the console window.\nThese functions are highly useful to check whether variables take on expected values!",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Control Structures</span>"
    ]
  },
  {
    "objectID": "05-control-structures.html#if",
    "href": "05-control-structures.html#if",
    "title": "\n5  Control Structures\n",
    "section": "",
    "text": "Note that every conditional statement (e.g. a_value &lt; 0) returns a logical value that is either TRUE or FALSE.\n\n\n\n\n\n\n\nExercise\n\n\n\nWhat result do you expect when you remove the negative sign in the code block above (a_value &lt;- 7)? To evaluate your expectation, run the code in RStudio.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe condition yields FALSE. The statement is not executed.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Control Structures</span>"
    ]
  },
  {
    "objectID": "05-control-structures.html#else",
    "href": "05-control-structures.html#else",
    "title": "\n5  Control Structures\n",
    "section": "\n5.2 Else",
    "text": "5.2 Else\nIn many cases, we want the interpreter to do something if the condition is satisfied or do something else, if the condition is not satisfied. In this case, we can use if together with else:\n\na_value &lt;- -7\nif (a_value &lt; 0){\n  cat(\"Negative\")\n} else {\n  cat(\"Positive\")\n}\n\nNegative\n\n\nIn the example above, the condition a_value &lt; 0 is TRUE, statement 1 cat(\"Negative\") is executed and statement 2 cat(\"Positive\") is ignored. If you change a_value to a positive value, the interpreter will ignore statement 1 and execute statement 2.\n\nNote that the statements are enclosed within curly brackets for clarity. While indentation doesn’t affect code execution in R, it’s essential for readability and maintaining code structure. However, inserting a line break before else returns an error. The reason for this behavior is explained in a forum thread.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Control Structures</span>"
    ]
  },
  {
    "objectID": "05-control-structures.html#code-blocks",
    "href": "05-control-structures.html#code-blocks",
    "title": "\n5  Control Structures\n",
    "section": "\n5.3 Code blocks",
    "text": "5.3 Code blocks\nConditional structures have a wide range of applications. Almost everything what a computer does requires an input. Each time you click a button the computer responds accordingly. The code that dictates the response typically has an if-else control structure or something very similar that tells the computer what to do depending on the input it got. Obviously in most cases the response won’t be defined by a single instruction, but a code block that is composed of multiple instructions. Code blocks allow encapsulating several statements in a single group. The condition in the following example yields TRUE and the code block is executed:\n\nfirst_value &lt;- 8\nsecond_value &lt;- 5\nif (first_value &gt; second_value) {\n  cat(\"First is greater than second\\n\") \n  difference &lt;- first_value - second_value\n  cat(\"Their difference is\", difference)\n}\n\nFirst is greater than second\nTheir difference is 3\n\n\nThe line cat(\"First is greater than second\\n\") prints text (string) and inserts a line break. The next line calculates the difference between first and second value. The third line in the code block concatenates two inputs (\"Their difference is\" and variable difference) and prints them to the console window.\n\nif and else are so called reserved words, meaning they cannot be used as variable names.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Control Structures</span>"
    ]
  },
  {
    "objectID": "05-control-structures.html#loops",
    "href": "05-control-structures.html#loops",
    "title": "\n5  Control Structures\n",
    "section": "\n5.4 Loops",
    "text": "5.4 Loops\nThe second family of control structures that we are going to discuss in this lesson are loops. Loops are a fundamental component of (procedural) programming. They allow repeating one or more instructions multiple times.\nThere are two main types of loops:\n\n\nconditional loops are executed as long as a defined condition holds true\n\nconstruct while\n\nconstruct repeat\n\n\n\n\ndeterministic loops are executed a pre-determined number of times\n\nconstruct for\n\n\n\n\n\n5.4.1 While and repeat\nThe while construct can be defined using the while reserved word, followed by a condition between simple brackets, and a code block. The instructions in the code block are re-executed as long as the result of the evaluation of the condition is TRUE.\n\ncurrent_value &lt;- 0\nwhile (current_value &lt; 3) {\n  cat(\"Current value is\", current_value, \"\\n\")\n  current_value &lt;- current_value + 1\n}\n\nCurrent value is 0 \nCurrent value is 1 \nCurrent value is 2 \n\n\n\n\n\n\n\n\nExercise\n\n\n\nGo through the example above and try to verbalize the consecutive steps.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThe variable current_value takes on a value of zero.\nThe condition of the while-loop returns TRUE.\nThe cat() function is executed and prints a text as well as current_value.\nThe variable current_value is incremented by +1.\nThe condition of the while-loop returns TRUE (current_value = 1), the code block is executed again (see 3 and 4).\n\ncurrent_value = 2, the code block is executed again (see 3 and 4).\n\ncurrent_value = 3, the condition returns FALSE, the loop ends.\n\n\n\n\nThe same procedure can alternatively be implemented by means of the repeat construct:\n\ncurrent_value &lt;- 0 \nrepeat { \n  cat(\"Current value is\", current_value, \"\\n\") \n  current_value = current_value + 1 \nif (current_value == 3){             # if (variable == 3)... \n  break                              # the loop will break!\n  }\n}\n\nCurrent value is 0 \nCurrent value is 1 \nCurrent value is 2 \n\n\nThe break statement is executed and stops (or ‘breaks’) the repeat loop (also applicable to while or for loops) once the variable current_value is equal to three.\n\n5.4.2 For\nThe for construct can be defined using the for reserved word, followed by the definition of an iterator. The iterator is a variable, which is temporarily assigned with the current element of a vector, as the construct iterates through all elements of the vector. This definition is followed by a code block, whose instructions are re-executed once for each element of the vector.\n\ncities &lt;- c(\"Derby\", \"Leicester\", \"Lincoln\", \"Nottingham\")\nfor (city in cities) {\n  cat(\"Do you live in \", city, \"?\\n\", sep=\"\")\n}\n\nDo you live in Derby?\nDo you live in Leicester?\nDo you live in Lincoln?\nDo you live in Nottingham?\n\n\nIn the first iteration of the for-loop, the text string \"Derby\" is assigned to the iterator city. The function cat() uses the iterator value as an input. In the second iteration, the text string \"Leicester\" is assigned to the iterator city … etc.\nThe code block below illustrates another example.\n\ncities &lt;- c(\"Derby\", \"Leicester\", \"Lincoln\", \"Nottingham\")\nletter_cnt &lt;- c()\nfor (city in cities) {\n  letter_cnt &lt;- c(letter_cnt, nchar(city))\n}\nprint(letter_cnt)\n\n[1]  5  9  7 10\n\n\nThe for-loop iterates over the elements in vector cities. The base function nchar() counts the number of letters of every city name and appends the count to a new vector letter_cnt.\n\nNote that with every iteration a new value is appended to the right side of the vector. The syntax for appending elements to a vector in R is…\n\n\nname vector &lt;- c(name vector, element to append)\n\nThere are some cases in which, for some reason, you just want to execute a certain sequence of steps a pre-defined number of times. In such cases, it is common practice to create a vector of integers on the spot. In the following example the for-loop is executed 3 times as it iterates over a vector composed of the three elements 1, 2, and 3 (vector is created on the spot by 1:3):\n\nfor (i in 1:3) {\n  cat(\"This is iteration number\", i, \":\\n\")\n  cat(\"    See you later!\\n\")\n}\n\nThis is iteration number 1 :\n    See you later!\nThis is iteration number 2 :\n    See you later!\nThis is iteration number 3 :\n    See you later!\n\n\n\n\n\n\n\n\nExercise\n\n\n\nReplace the vector 1:3 by a vector 3:5. What is different?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe for-loop is still executed 3 times.\nHowever, the iterator ‘i’ returns the values 3, 4, and 5.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Control Structures</span>"
    ]
  },
  {
    "objectID": "05-control-structures.html#loops-with-conditional-statements",
    "href": "05-control-structures.html#loops-with-conditional-statements",
    "title": "\n5  Control Structures\n",
    "section": "\n5.5 Loops with conditional statements",
    "text": "5.5 Loops with conditional statements\nHaving explored both types of control structures, conditional statements and loops, we can combine these structures. R, as most other programming languages, allows you to include conditional statements within a loop or a loop within a conditional statement.\nA simple example is this bit of code that defines a countdown:\n\n# Example: countdown!\nfor (i in 3:0) {\n  if (i == 0) {\n    cat(\"Go!\\n\")\n  } else {\n    cat(i, \"\\n\")\n  }\n}\n\n3 \n2 \n1 \nGo!\n\n\nThe deterministic loop runs 4 time on the values 3, 2, 1, and 0. If the iterator i takes on a value of 0 the print \"Go!\" otherwise print the current value of the iterator i. The result will be 3, 2, 1, Go!\n\nSee another example!\n\n\n\n\n\nlibrary(tidyverse)\n\ncities &lt;- c(\"Salzburg\", \"Linz\", \"Wien\", \"Eisenstadt\", \"Innsbruck\", \"Graz\")\n\nfor (city in cities){\n  if (str_starts(city, \"S\")){\n    print(\"City name starts with S\")\n  } else{\n    print(\"City name starts with other letter\")\n  }\n  \n}\n\n[1] \"City name starts with S\"\n[1] \"City name starts with other letter\"\n[1] \"City name starts with other letter\"\n[1] \"City name starts with other letter\"\n[1] \"City name starts with other letter\"\n[1] \"City name starts with other letter\"\n\n\nWe need to load the library ‘tidyverse’ to make use of the function ‘str_starts()’. You may have to install ‘tidyverse’ (see Libraries in lesson core Concepts).\ncities is a vector of strings that includes the names of some Austrian federal capitals. The for-loop iterates over these vector elements. The function str_starts takes the value of the iterator city as well as a string \"S\" as inputs. If the city starts with letter S, the function returns TRUE and \"City name starts with S\" is printed to the console window, otherwise the function returns FALSE and \"City name starts with other letter\" is printed.\n\n\n\n5.5.1 Exercise: loops with conditional statements\n\n\n\n\n\n\nExercise\n\n\n\nAs a last exercise in this lesson, you will implement code that iterates over a two-dimensional SpatRaster Object and counts values in the raster grid that exceed a certain threshold.\n\n\n\nCreate a SpatRaster Object r with 20 rows and 20 columns and assign random values in a range between 0 and 1. Use the function runif as random value generator. It is recommended to make use of code snippets from lesson Spatial Data Structures.\nDefine a variable named cnt and assign a value of 0 to it.\nIterate over the SpatRaster Object r by means of a nested for-loop:\n\n\nfor(row in c(1:nrow(r))) {               \n  for(col in 1:ncol(r)) {                 \n     \n    print(r[row, col])             \n \n  }\n}\n\n\nAdd a conditional statement within the for-loops that increases the value of cnt by one, given that r[row, col] is &gt; 0.5. Eventually, variable cnt will hold the number of raster grid values that exceed a threshold of 0.5.\n\nTask 1:\nTry to find out, in what order the nested-for-loop-structure iterates over the SpatRaster Object r.\nTask 2:\nSpatRaster Object r contains randomly generated values in a range between 0 and 1. Change the threshold value in your conditional statement a couple of times to investigate the distribution of random values. Does the algorithm in function runif draw values from a normal or from a uniform distribution?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSee our code.\nAnswer 1:\nThe nested for-loop-structure iterates through the SpatRaster Object in a row-major order. It starts its iteration in the leftmost cell of row 1, moves to the right through columns (e.g., col1, col2) until it reaches the last column in row 1. Then, it moves down to row 2 and repeats the process for each row in a similar manner. This row-major order ensures that it covers each element of the SpatRaster Object row by row.\nAnswer 2:\nA threshold value of 0.9 returns a count of about 40, which indicates that about 10% of values are greater than 0.9. It seems like random values are uniformly distributed in a range between 0 and 1. Other R functions like rnorm() generate numbers from a normal distribution.\n\n\n\nAs an alternative, the same functionality may be implemented without loop. Instead, operations can be vectorized:\n\nlibrary(terra)\n\nr &lt;- terra::rast(ncol=20, nrow=20)\nterra::values(r) &lt;- stats::runif(terra::ncell(r),0,1)\n\n# Get values of SpatRaster Object `r` as matrix\nrm &lt;- terra::values(r)\n# Create logic vector by condition\nlog_vec &lt;- rm[,1] &gt; 0.5\n# Get length of TRUE values (grid value &gt; 0.5) in logic vectors\nlength(log_vec[log_vec== TRUE])\n\n[1] 199\n\n\nIt is apparent from the code example above that avoiding loops in R code makes code shorter and increases performance. Increase the size of your raster grid (e.g. ncol=100, nrow=100) and test respective code realizations to investigate the difference in code performance. More information on vectorization and parallelization of operations in R can be found here.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Control Structures</span>"
    ]
  },
  {
    "objectID": "06-functions.html",
    "href": "06-functions.html",
    "title": "\n6  Functions\n",
    "section": "",
    "text": "6.1 Defining Functions\nIn previous lessons, we’ve used various functions (like cat(), print()) without delving deeply into their mechanics. This lesson aims to demystify functions by guiding you through the creation and understanding of custom functions. We’ll also explore the concepts of global and local variable scopes.\nDefining a function in R follows a syntax similar to variable assignments or conditional statements. We start with an identifier (e.g., add_one) on the left of an assignment operator (&lt;-).\nThe body of the function follows, beginning with the keyword function, the parameter(s) enclosed in parentheses (e.g., input_value), and the function’s body within curly braces. The value of the last executed statement is automatically returned:\nadd_one &lt;- function (input_value) {\n  output_value &lt;- input_value + 1\n  return(output_value) # Explicit return\n}\nadd_one(3)\n\n[1] 4\nCall this function by specifying its identifier and the necessary parameter(s). For instance, in our example add_one(3) returns 4.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "06-functions.html#defining-functions",
    "href": "06-functions.html#defining-functions",
    "title": "\n6  Functions\n",
    "section": "",
    "text": "6.1.1 Understanding the return() Function\nIn R, functions automatically return the value of the last expression evaluated. The decision to use the return() function explicitly or rely on implicit return is often guided by the specific context of your work, especially in fields like GIS.\nImplicit Return:\nIn R, if return() is not explicitly used, the function returns the result of the last line of code executed. This is often sufficient, especially in simpler functions. For example:\n\nadd_one &lt;- function (input_value) {\n  input_value + 1\n}\nadd_one(3)  # Returns 4\n\n[1] 4\n\n\nExplicit Return:\nUsing return(), you can explicitly specify what the function should return. This approach is particularly useful when the function has complex logic, or when you need to return a value before reaching the end of the function. For example:\n\nadd_one &lt;- function (input_value) {\n  result &lt;- input_value + 1\n  return(result)\n}\nadd_one(3)  # Also returns 4\n\n[1] 4\n\n\nIn both examples, calling add_one(3) returns 4. The explicit use of return() in the second example provides clarity about the intended output of the function.\nThe choice between implicit and explicit return in R functions can depend on various factors, including the complexity of the function and the coding standards of your team or organization. In GIS and similar fields, it’s important to understand both approaches and adapt to the coding practices of your workplace. Always consider the context and ask, “What is the standard here?”\n\nEarly Return Example:\nSometimes, it’s necessary to exit a function early, for instance in error handling:\n\nsafe_divide &lt;- function (numerator, denominator) {\n  if (denominator == 0) {\n    return(\"Error: Division by zero\")\n  }\n  numerator / denominator\n}\nsafe_divide(10, 0)  # Returns \"Error: Division by zero\"\n\n[1] \"Error: Division by zero\"\n\n\nSo, adapting to the specific requirements of your project and following established coding standards are key to effective and collaborative work.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "06-functions.html#more-parameters",
    "href": "06-functions.html#more-parameters",
    "title": "\n6  Functions\n",
    "section": "\n6.2 More Parameters",
    "text": "6.2 More Parameters\nFunctions can have multiple parameters, separated by commas.\n\nA function expects as many input values as there are parameters specified in its definition; otherwise, an error is triggered.\n\nThe area_rectangle function includes two parameters (height and width), calculates the area by multiplying the inputs, and returns the area as a numeric value:\n\narea_rectangle &lt;- function (height, width) {\n  return(height * width)\n}\narea_rectangle(3, 2)\n\n[1] 6\n\n\nDefault parameters can enhance a function’s flexibility. For instance, you can modify the area_rectangle function parameters as (height, width = 3). Now, with only one input, the function should return YOUR INPUT * 3. Specifying both parameters overrides the default width. Here’s how it looks:\n\narea_rectangle &lt;- function (height, width = 3) {\n  return(height * width)\n}\n\n# Calling with one parameter uses the default width of 3\narea_rectangle(3)  # Returns 9\n\n[1] 9\n\n# Calling with two parameters uses the specified width\narea_rectangle(3, 2)  # Returns 6\n\n[1] 6\n\n\nThis approach allows the function to be more flexible and adaptable to different use cases.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "06-functions.html#returning-multiple-values",
    "href": "06-functions.html#returning-multiple-values",
    "title": "\n6  Functions\n",
    "section": "\n6.3 Returning Multiple Values",
    "text": "6.3 Returning Multiple Values\nTo return multiple values from a function, encapsulate them in a list. For example, function rectangle_metrics calculates and returns both the area and perimeter of a rectangle:\n\nrectangle_metrics &lt;- function (height, width) {\n  area &lt;- height * width\n  perimeter &lt;- 2 * (height + width)\n  \n  return(list(area = area, perimeter = perimeter))\n}\n\nRetrieve these values using list indices [[1]] and [[2]], or by simply using their names:\n\nmetrics &lt;- rectangle_metrics(3, 2)\n\n# Using list indices\ncat(\"Area (list index): \", metrics[[1]], \"\\n\")\n\nArea (list index):  6 \n\ncat(\"Perimeter (list index): \", metrics[[2]], \"\\n\")\n\nPerimeter (list index):  10 \n\n# Using names\ncat(\"Area (name): \", metrics$area, \"\\n\")\n\nArea (name):  6 \n\ncat(\"Perimeter (name): \", metrics$perimeter, \"\\n\")\n\nPerimeter (name):  10 \n\n\n\n\n\n\n\n\nTip\n\n\n\nUpon defining a function in R, it appears in the Environment Window of RStudio, similar to a variable. When invoked, the R interpreter executes the function stored in memory.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "06-functions.html#functions-and-control-structures",
    "href": "06-functions.html#functions-and-control-structures",
    "title": "\n6  Functions\n",
    "section": "\n6.4 Functions and Control Structures",
    "text": "6.4 Functions and Control Structures\nFunctions can contain loops and conditional statements. Here’s a function that uses a loop to calculate the factorial of a number (e.g., factorial of 3 = 1 * 2 * 3 = 6):\n\nfactorial &lt;- function (input_value) {\n  result &lt;- 1\n  for (i in 1:input_value) {\n    cat(\"Current:\", result, \" | i:\", i, \"\\n\")\n    result &lt;- result * i\n  }\n  return(result)\n}\nfactorial(3)\n\nCurrent: 1  | i: 1 \nCurrent: 1  | i: 2 \nCurrent: 2  | i: 3 \n\n\n[1] 6\n\n\nThe function takes a single numeric value as input, defines a variable named result that is equal to ‘1’ and then creates a loop over all the numbers from 1 (variable result) to the input_value. In the loop, the current value of result is multiplied by the value of the iterator i.\n\nThough technically possible, defining a function within loops or conditional statements is generally avoided.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "06-functions.html#scope",
    "href": "06-functions.html#scope",
    "title": "\n6  Functions\n",
    "section": "\n6.5 Scope",
    "text": "6.5 Scope\nFunction parameters are internal variables acting as a bridge between the function and its environment. They are ‘visible’ only within the function’s scope.\n\nThe scope of a variable refers to the code region where the variable is accessible.\n\nUnderstanding scope is crucial for several reasons:\n\n\nAvoiding Variable Conflicts: Variables with the same name can exist in different scopes without conflicting with each other. This prevents unexpected behavior caused by variable name overlaps.\n\nPredictable Behavior: Knowing whether a variable is global or local helps predict its behavior and influence within your script. It clarifies where and how variables can be accessed or modified.\n\nResource Management: Local variables are often cleared from memory once the function execution is complete, aiding in efficient resource management in larger scripts or applications.\n\nIn R, the scope of variables is defined as follows:\n\n\nGlobal Variables: Defined outside of any function and accessible anywhere in the script, including within functions.\n\nLocal Variables: Defined within a function and accessible only in that function’s scope. This includes function parameters, which act as internal variables bridging the function and its environment.\n\nIn the following example, x_value is a global variable, while new_value and input_value are local to the times_x function. Accessing new_value or input_value outside times_x would result in an error, but x_value can be referred to within the function:\n\nx_value &lt;- 10\ntimes_x &lt;- function (input_value) {\n  new_value &lt;- input_value * x_value\n  return(new_value)\n}\ntimes_x(2)\n\n[1] 20\n\n\nUnderstanding these distinctions is essential for writing robust and error-free R code, especially in more complex data analysis or GIS projects.\n\n\n\n\n\n\nExercise\n\n\n\nReferring to external global variables in a function is possible but can be dangerous. At the time of execution, one cannot be sure what the value of the global variable is. For instance, other processes might have changed its value, which affects the behavior of the function. In order to fix this problem, define the variable x_value as a default parameter of function times_x.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ntimes_x &lt;- function (input_value, x_value = 10) {\n  return(input_value * x_value)\n}\n\n\n\n\n\nThe previous lessons have introduced basic concepts of R programming. The Base R Cheatsheet offers a concise summary of key operations.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "07-data-manipulation.html",
    "href": "07-data-manipulation.html",
    "title": "7  Data Manipulation",
    "section": "",
    "text": "7.1 Preparation\nIn most instances, the structure of the available data will not meet the specific requirements needed to perform the analyses you are interested in. Data analysts typically spend most of their time cleaning, filtering, restructuring data as well as harmonizing and joining data from different sources.\nIn this lesson, you will learn about the fundamental data wrangling operations using the dplyr library from the Tidyverse suite. You’ll also be introduced to tibbles, a modern take on data frames in R. Tibbles are lightweight and work seamlessly within the Tidyverse ecosystem. For more on tibbles, see Tibbles in R for Data Science.\nIf not already installed on your machine, install both the tidyverse and nycflights13 libraries. The installation of libraries is explained in chapter “libraries” of lesson Core Concepts.\nThe code below loads a sample dataset (a tibble) from the library nycflights13 into the variable flights_from_nyc. We will use this sample data in this lesson.\nlibrary(nycflights13)\nflights_from_nyc &lt;- nycflights13::flights\nThe operator :: is used to indicate that the function flights (that returns our sample dataset) is situated within the library nycflights13. This helps avoid ambiguities when functions from different loaded libraries have the same names.\nIn order to run the following data wrangling examples on your machine, add both lines above as well as the code snippets provided in the upcoming examples to a new R script file.\nOnce you have loaded the flights table, open the Environment Tab in RStudio and double-click variable flights_from_nyc to inspect the variable contents.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "07-data-manipulation.html#preparation",
    "href": "07-data-manipulation.html#preparation",
    "title": "7  Data Manipulation",
    "section": "",
    "text": "Alternatively, you may inspect flights_from_nyc by writing it to the console.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "07-data-manipulation.html#data-manipulation",
    "href": "07-data-manipulation.html#data-manipulation",
    "title": "7  Data Manipulation",
    "section": "\n7.2 Data manipulation",
    "text": "7.2 Data manipulation\nThe dplyr library provides a number of functions to investigate the basic characteristics of inputs. For instance, the function count() can be used to count the number of rows of a data frame or tibble. The code below uses flights_from_nyc as input to this function. The returned row count n is represented in a table.\n\nlibrary(tidyverse)\nlibrary(knitr)\n\nflights_from_nyc %&gt;%\n  dplyr::count() %&gt;%\n  knitr::kable()\n\n\n\nn\n\n\n336776\n\n\n\n\nIn the example above, we use the pipe operator %&gt;%, a key feature of magrittr and tidyverse syntax. The operator links a sequence of analysis steps, passing flights_from_nyc through count() and then kable(). More specifically, the pipe operator passes the left-hand object as the first argument to the right-hand function. This makes the code simpler and more readable.\nThe same result could be achieved with a traditional approach using an intermediate variable or by nesting functions. However, these methods can make the code less readable compared to using the pipe operator:\nIntermediate Variable:\n\ncnt &lt;- dplyr::count(flights_from_nyc)\nknitr::kable(cnt)\n\nNested Function:\n\nknitr::kable(dplyr::count(flights_from_nyc))\n\nThe result is the same. However, nested structures are harder to read. Accordingly, the pipe operator is a powerful tool to simplify your code.\nThe function count() can also be used to count the number of rows of a table that has the same value for a given column, usually representing a category.\nIn the example below, the column name origin is provided as an argument to the function count(), so rows representing flights from the same origin are counted together – EWR is the Newark Liberty International Airport, JFK is the John F. Kennedy International Airport, and LGA is LaGuardia Airport.\n\nflights_from_nyc %&gt;%\n  dplyr::count(origin) %&gt;%\n  knitr::kable()\n\n\n\norigin\nn\n\n\n\nEWR\n120835\n\n\nJFK\n111279\n\n\nLGA\n104662\n\n\n\n\n\nNotice how the code becomes more readable with each operation on a new line after %&gt;%.\nFor a deeper understanding of pipe operators, check out the following video.\n\n\n\n\n\n\nFigure 7.1: Worked examples - magrittr and Base R pipe operators\n\n\n\nDownload Worked Example\n\n\n\n\n\n\n\nTip\n\n\n\nTo change the argument placement, use . within the function call. For example, you could explicitly pass variable flights_from_nyc as a first argument by dplyr::count(., origin). Passing the variable as a second argument dplyr::count(origin, .) will return an error as the data frame is expected as a first input to function count.\n\n\n\n7.2.1 Summarise\nTo carry out more complex aggregations, the function summarise() can be used in combination with the function group_by() to summarise the values of the rows of a data frame or tibble. Rows having the same value for a selected column (in the example below, the same origin) are grouped together, then values are aggregated based on the defined function (using one or more columns in the calculation).\nIn the example below, the function mean() is applied to the column distance to calculate a new column mean_distance_traveled_from (the mean distance travelled by flights starting from each airport).\n\nflights_from_nyc %&gt;%\n  dplyr::group_by(origin) %&gt;%\n  dplyr::summarise(\n    mean_distance_traveled_from = mean(distance)\n  ) %&gt;%\n  knitr::kable()\n\n\n\norigin\nmean_distance_traveled_from\n\n\n\nEWR\n1056.7428\n\n\nJFK\n1266.2491\n\n\nLGA\n779.8357\n\n\n\n\n\nThe results show that the average distance covered by flights starting from JFK is significantly higher than flights departing from LGA or EWR.\n\n7.2.2 Select and filter\nThe function select() can be used to select a subset of columns. For instance in the code below, the function select() is used to select the columns origin, dest, and dep_delay. The function slice_head is used to include only the first n rows in the output.\n\nflights_from_nyc %&gt;%\n  dplyr::select(origin, dest, dep_delay) %&gt;%\n  dplyr::slice_head(n = 5) %&gt;%\n  knitr::kable()\n\n\n\norigin\ndest\ndep_delay\n\n\n\nEWR\nIAH\n2\n\n\nLGA\nIAH\n4\n\n\nJFK\nMIA\n2\n\n\nJFK\nBQN\n-1\n\n\nLGA\nATL\n-6\n\n\n\n\n\nThe function filter() can instead be used to filter rows based on a specified condition. In the example below, the output of the filter step only includes the rows where the value of month is 11 (i.e., the eleventh month, November).\n\nflights_from_nyc %&gt;%\n  dplyr::select(origin, dest, year, month, day, dep_delay) %&gt;%\n  dplyr::filter(month == 11) %&gt;%\n  dplyr::slice_head(n = 5) %&gt;%\n  knitr::kable()\n\n\n\norigin\ndest\nyear\nmonth\nday\ndep_delay\n\n\n\nJFK\nPSE\n2013\n11\n1\n6\n\n\nJFK\nSYR\n2013\n11\n1\n105\n\n\nEWR\nCLT\n2013\n11\n1\n-5\n\n\nLGA\nIAH\n2013\n11\n1\n-6\n\n\nJFK\nMIA\n2013\n11\n1\n-3\n\n\n\n\n\nNotice how filter is used in combination with select. All functions in the dplyr library can be combined, in any other order that makes logical sense. However, if the select step didn’t include month, that same column couldn’t have been used in the filter step.\n\n7.2.3 Mutate\nThe function mutate() can be used to add a new column to an output table. The mutate step in the code below adds a new column air_time_hours to the table obtained through the pipe, that is the flight air time in hours, dividing the flight air time in minutes by 60.\n\nflights_from_nyc %&gt;%\n  dplyr::select(flight, origin, dest, air_time) %&gt;%\n  dplyr::mutate(\n    air_time_hours = air_time / 60\n  ) %&gt;%\n  dplyr::slice_head(n = 5) %&gt;%\n  knitr::kable()\n\n\n\nflight\norigin\ndest\nair_time\nair_time_hours\n\n\n\n1545\nEWR\nIAH\n227\n3.783333\n\n\n1714\nLGA\nIAH\n227\n3.783333\n\n\n1141\nJFK\nMIA\n160\n2.666667\n\n\n725\nJFK\nBQN\n183\n3.050000\n\n\n461\nLGA\nATL\n116\n1.933333\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nRun the mutate example above in a new script and replace dplyr::mutate with dplyr::transmute. What happens to your results?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe transmute function creates a new data frame containing only column air_time_hours. The function drops other table columns.\n\n\n\n\n7.2.4 Arrange\nThe function arrange() sorts a tibble or data frame in ascending order based on the values in the specified column. If a negative sign is specified before the column name, the descending order is used. The code below would produce a table showing all the rows when ordered by descending order of air time.\n\nflights_from_nyc %&gt;%\n  dplyr::select(origin, dest, air_time) %&gt;%\n  dplyr::arrange(-air_time) %&gt;%\n  dplyr::slice_head(n = 5) %&gt;%\n  knitr::kable()\n\n\n\norigin\ndest\nair_time\n\n\n\nEWR\nHNL\n695\n\n\nJFK\nHNL\n691\n\n\nJFK\nHNL\n686\n\n\nJFK\nHNL\n686\n\n\nJFK\nHNL\n683\n\n\n\n\n\nIn the examples above, we have used slice_head to present only the first n rows in a table, based on the existing order. The result shows that among domestic flights that depart from New York in 2013, the flight from Newark airport (EWR) to Honululu Airport (HNL) has the longest travel time.\n\n7.2.5 Exercise: data manipulation\n\n\n\n\n\n\nExercise\n\n\n\nThe Food and Agriculture Organization (FAO) is a specialized agency of the United Nations that leads international efforts to defeat hunger. On their Website they provide comprehensive datasets on global crop and livestock production. Your task in this exercise is to create a table that shows national African sorghum production in 2019.\n\n\n\nCreate an RScript and install or load the libraries tidyverse and knitr, if not done yet.\nBulk download African Crop and Livestock Production data as CSV:\n\n\n\n\n\n\nFigure 7.2: FAO Data Download\n\n\n\nRead data from comma-separated CSV (\"Production_Crops_Livestock_E_Africa_NOFLAG.csv\") into your Script.\n\nfao_data &lt;- read.csv(directory as string, header = TRUE, sep = \",\")\n\nUse the pipe operator to perform the following operations:\n\nSelect columns Area, Item, Element, Unit and Y2019\n\nFilter rows that contain sorghum production (Item == \"Sorghum\" & Element == \"Production\")\nSort the table based on yield in descending order (arrange)\nremove rows including No Data by means of function drop_na()\n\nrender the table using the function kable() of library knitr\n\n\n\n\nSee our solution!\nNote on Encoding: When working with datasets from various sources, you may encounter encoding errors. If such an issue arises, a simple approach is to specify the encoding in the read.csv() function. For instance, you can use read.csv(..., fileEncoding = \"latin1\"). However, the best case would be, to check the original encoding of the data beforehand and match it in your read.csv() command. Encoding can be tricky, and while it’s outside the scope of this exercise, being aware of it is crucial in data analysis.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "07-data-manipulation.html#join",
    "href": "07-data-manipulation.html#join",
    "title": "7  Data Manipulation",
    "section": "\n7.3 Join",
    "text": "7.3 Join\nA join operation combines two tables into one by matching rows that have the same values in a specified column. This operation is usually executed on columns containing identifiers, which are matched through different tables containing different data about the same real-world entities.\nFor instance, the table created below (data frame city_telephone_prexix) contains the telephone prefixes of three cities.\n\ncity_telephone_prexix &lt;- data.frame(\n    city = c(\"Leicester\", \"Birmingham\", \"London\"),\n    telephon_prefix = c(\"0116\", \"0121\", \"0171\")\n  ) %&gt;%\n  tibble::as_tibble()\n\ncity_telephone_prexix %&gt;%\n  knitr::kable()\n\n\n\ncity\ntelephon_prefix\n\n\n\nLeicester\n0116\n\n\nBirmingham\n0121\n\n\nLondon\n0171\n\n\n\n\n\nThat information can be combined with the data present in a second table city_info_wide (see below) through the join operation on the columns containing the city names.\n\ncity_info_wide &lt;- data.frame(\n    city = c(\"Leicester\", \"Nottingham\"),\n    population = c(329839, 321500),\n    area = c(73.3, 74.6),\n    density = c(4500, 4412)\n  ) %&gt;%\n  tibble::as_tibble()\n\ncity_info_wide %&gt;%\n knitr::kable()\n\n\n\ncity\npopulation\narea\ndensity\n\n\n\nLeicester\n329839\n73.3\n4500\n\n\nNottingham\n321500\n74.6\n4412\n\n\n\n\n\n\nNote that data frames in the code above are converted to tibbles. This step is necessary because the kable() function requires tibbles as input.\n\nJoins between two tables can be executed by different join operations. The dplyr library offers join operations, which correspond to SQL joins illustrated in the image below.\n\n\n\n\n\nFigure 7.3: Join types. Source: R for Geographic Data Science\n\n\nPlease take your time to understand the examples below that show different realizations of joins between table city_telephone_prexix and table city_info_wide.\nThe first four examples execute the exact same full join operation using three different syntaxes: with or without using the pipe operator and specifying the by argument or not. Note that all those approaches to writing the join are valid and produce the same result. The choice about which approach to use will depend on the code you are writing. In particular, you might find it useful to use the syntax that uses the pipe operator when the join operation is itself only one stem in a series of data manipulation steps. Using the by argument is usually advisable unless you are certain that you aim to join two tables with all and exactly the column that have the same names in the two table.\nNote how the result of the join operations is not saved to a variable. The function knitr::kable is added after each join operation through a pipe %&gt;% to display the resulting table in a nice format.\n\n\n# Option 1: without using the pipe operator\n# full join verb\ndplyr::full_join(\n    # left table\n    city_info_wide,\n    # right table\n    city_telephone_prexix,\n    # columns to match\n    by = c(\"city\" = \"city\")\n  ) %&gt;%\n  knitr::kable()\n\n\n\ncity\npopulation\narea\ndensity\ntelephon_prefix\n\n\n\nLeicester\n329839\n73.3\n4500\n0116\n\n\nNottingham\n321500\n74.6\n4412\nNA\n\n\nBirmingham\nNA\nNA\nNA\n0121\n\n\nLondon\nNA\nNA\nNA\n0171\n\n\n\n\n\n\n# Option 2: without using the pipe operator\n#   and without using the argument \"by\"\n#   as columns have the same name\n#   in the two tables.\n# Same result as Option 1\n# full join verb\ndplyr::full_join(\n    # left table\n    city_info_wide,\n    # right table\n    city_telephone_prexix\n  ) %&gt;%\n  knitr::kable()\n\n\n\ncity\npopulation\narea\ndensity\ntelephon_prefix\n\n\n\nLeicester\n329839\n73.3\n4500\n0116\n\n\nNottingham\n321500\n74.6\n4412\nNA\n\n\nBirmingham\nNA\nNA\nNA\n0121\n\n\nLondon\nNA\nNA\nNA\n0171\n\n\n\n\n\n\n\n# Option 3: using the pipe operator\n#   and without using the argument \"by\"\n#   as columns have the same name\n#   in the two tables.\n# Same result as Option 1 and 2\n# left table\ncity_info_wide %&gt;%\n  # full join verb\n  dplyr::full_join(\n    # right table\n    city_telephone_prexix\n  ) %&gt;%\n  knitr::kable()\n\n\n\ncity\npopulation\narea\ndensity\ntelephon_prefix\n\n\n\nLeicester\n329839\n73.3\n4500\n0116\n\n\nNottingham\n321500\n74.6\n4412\nNA\n\n\nBirmingham\nNA\nNA\nNA\n0121\n\n\nLondon\nNA\nNA\nNA\n0171\n\n\n\n\n\n\n# Option 4: using the pipe operator\n#   and using the argument \"by\".\n# Same result as Option 1, 2 and 3\n# left table\ncity_info_wide %&gt;%\n  # full join verb\n  dplyr::full_join(\n    # right table\n    city_telephone_prexix,\n    # columns to match\n    by = c(\"city\" = \"city\")\n  ) %&gt;%\n  knitr::kable()\n\n\n\ncity\npopulation\narea\ndensity\ntelephon_prefix\n\n\n\nLeicester\n329839\n73.3\n4500\n0116\n\n\nNottingham\n321500\n74.6\n4412\nNA\n\n\nBirmingham\nNA\nNA\nNA\n0121\n\n\nLondon\nNA\nNA\nNA\n0171\n\n\n\n\n\n\n\n# Left join\n# Using syntax similar to Option 1 above\n# left join\ndplyr::left_join(\n    # left table\n    city_info_wide, \n    # right table\n    city_telephone_prexix,\n    # columns to match\n    by = c(\"city\" = \"city\")\n  ) %&gt;%\n  knitr::kable()\n\n\n\ncity\npopulation\narea\ndensity\ntelephon_prefix\n\n\n\nLeicester\n329839\n73.3\n4500\n0116\n\n\nNottingham\n321500\n74.6\n4412\nNA\n\n\n\n\n\n\n# Right join\n# Using syntax similar to Option 2 above\n# right join verb\ndplyr::right_join(\n    # left table\n    city_info_wide, \n    # right table\n    city_telephone_prexix\n  ) %&gt;%\n  knitr::kable()\n\n\n\ncity\npopulation\narea\ndensity\ntelephon_prefix\n\n\n\nLeicester\n329839\n73.3\n4500\n0116\n\n\nBirmingham\nNA\nNA\nNA\n0121\n\n\nLondon\nNA\nNA\nNA\n0171\n\n\n\n\n\n\n\n# Inner join\n# Using syntax similar to Option 3 above\n# left table\ncity_info_wide %&gt;%\n  # inner join\n  dplyr::inner_join(\n    # right table\n    city_telephone_prexix\n  ) %&gt;%\n  knitr::kable()\n\n\n\ncity\npopulation\narea\ndensity\ntelephon_prefix\n\n\nLeicester\n329839\n73.3\n4500\n0116\n\n\n\n\n\nCompare the results of respective full, left, right and inner join operations. Turn to the discussion forum in case you need further explanations.\n\n\n7.3.1 Exercise: join\n\n\n\n\n\n\nExercise\n\n\n\nIn the previous exercise, we created a table that shows national African sorghum production in 2019. In this exercise we will join crop production statistics with a table that contains national boundaries and visualize sorghum production quantities in a simple map.\n\n\n\nCreate an RScript and install and load the libraries tidyverse, knitr, ggplot2 and maps, if not done yet.\nCopy the code from the previous exercise into your new RScript. Note that the result of the pipe operations is not saved to a variable. Save it to a variable.\nUse the ggplot2 function map_data to convert the built in sample dataset world (comes with library maps) to a data frame:\n\nworld_ctry &lt;- map_data(\"world\") \n\nInspect the structure of this data frame. Every row represents a node (defined by long/lat) of a polygon feature (national boundaries).\nJoin tables (left table: geographic features, right table: sorghum production statistics) based on country names. Make sure to choose a join method (full_join, inner_join, left_join or right_join) that allows for retaining all the geographic features.\n\nOur exercise solution creates a simple output map. Don’t worry in Lesson Data Visualization we will cover visualization methods in more detail.\n\n\n\n\n\n\nTip\n\n\n\nTake a look at the dplyr Cheatsheet which shows the most important dplyr operations at a glance.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "08-read-write.html",
    "href": "08-read-write.html",
    "title": "8  Read and Write Data",
    "section": "",
    "text": "8.1 Read and write tabular data\nIn previous exercises we have read data from a CSV file into our script. Similarly we can also write code outputs to file.\nIn this lesson you will learn to read and write plain-text, spatial vector and raster file formats. Moreover, we will retrieve online data by means of a data API.\nA series of formats are based on plain-text files.\nFor instance…\nThe readr library (also part of Tidyverse) provides a series of functions that can be used to load from and save to such data formats. For instance, the read_csv function reads a comma-delimited CSV file from the path provided as the first argument.\nThe code example below reads a CSV file that contains global fishery statistics provided by the World Bank and queries Norwegian entries. The function write_csv writes these entries to a new CSV file.\nlibrary(tidyverse)\n\nfishery_data &lt;- readr::read_csv(\"data/capture-fisheries-vs-aquaculture.csv\")\n# print(fishery_data)\n\n# print(typeof(fishery_data$))\n\nfishery_data %&gt;%\n  dplyr::filter(Entity == \"Norway\") %&gt;%\n  readr::write_csv(\"data/capture-fisheries-vs-aquaculture-norway.csv\", append=FALSE) %&gt;%\n  \n  dplyr::slice_head(n = 3) %&gt;%\n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\nEntity\nCode\nYear\nAquaculture production (metric tons)\nCapture fisheries production (metric tons)\n\n\n\nNorway\nNOR\n1960\n1900\n1609362\n\n\nNorway\nNOR\n1961\n900\n1758413\n\n\nNorway\nNOR\n1962\n200\n1572913\nIn order to run the script, download the CSV file. Then copy and run the code in a new R-script.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Read and Write Data</span>"
    ]
  },
  {
    "objectID": "08-read-write.html#read-and-write-tabular-data",
    "href": "08-read-write.html#read-and-write-tabular-data",
    "title": "8  Read and Write Data",
    "section": "",
    "text": "comma-separated values files .csv\n\nsemi-colon-separated values files .csv\n\ntab-separated values files .tsv\n\nother formats using custom delimiters\nfix-width files .fwf\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nOther important packages for reading tabular data are readxl for Excel (.xls and .xlsx) and haven for SPSS, Stata and SAS data.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Read and Write Data</span>"
    ]
  },
  {
    "objectID": "08-read-write.html#read-and-write-vector-data",
    "href": "08-read-write.html#read-and-write-vector-data",
    "title": "8  Read and Write Data",
    "section": "\n8.2 Read and write vector data",
    "text": "8.2 Read and write vector data\nThe library sf makes it easy to read and write vector datasets such as shapefiles.\n\nRemember: In lesson Spatial Data Structures, we used the sf library to build simple feature vector geometries from scratch, investigate their properties and assign and transform coordinate reference systems. Now we use the same library to load sf-objects from file.\n\nIn order to load vector data in an R-Script, we can use the function st_read(). In the code block below, a shapefile (North Carolina sample data) is loaded and assigned to a variable nc.\nThe next line creates a basic map in sf by means of plot(). By default this creates a multi-panel plot, one sub-plot for each variable (attribute) of the sf-object.\n\nlibrary(sf)\n\nnc &lt;- sf::st_read(\"data/nc.shp\")\n\nplot(nc)\n\n\n\n\n\n\n\nAs you have learned in lesson Spatial Data Structures, sf represents features as records in data-frame-like structures with an additional geometry list-column. The example below renders three features (rows) of variable nc including the geometry column of type MULTIPOLYGON (see Simple feature geometry types) as well as the attributes AREA (feature area) and NAME (name of county):\n\nnc %&gt;%\n  dplyr::select(AREA, NAME, geometry) %&gt;%\n  dplyr::slice_head(n = 3) %&gt;%\n  knitr::kable()\n\n\n\nAREA\nNAME\ngeometry\n\n\n\n0.114\nAshe\nMULTIPOLYGON (((-81.47276 3…\n\n\n0.061\nAlleghany\nMULTIPOLYGON (((-81.23989 3…\n\n\n0.143\nSurry\nMULTIPOLYGON (((-80.45634 3…\n\n\n\n\n\nsf also includes a number of operations to manipulate the geometry of features such as st_simplify:\n\nsf::st_simplify(nc) %&gt;%\n  plot(., max.plot = 1)\n\n\n\n\n\n\n\n\nYou may have recognized that a dot (.) is used as a parameter in the function plot(). The dot represents the piped value. In the example above the dot is used to define the simplified geometry of nc as first parameter of function plot() and max.plot = 1 as the second.\n\nIn the next example, the st_geometry() retrieves the geometry attribute from variable nc, function st_centroid() calculates the centroid of the polygon geometry (counties) and function st_write writes the centroid point geometry to file.\n\nsf::st_geometry(nc) %&gt;%\n  sf::st_centroid() %&gt;%\n  sf::st_write(\"data/nc-centroids.shp\", delete_dsn = TRUE) %&gt;%\n  plot(pch = 3, col = 'red') \n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe online book Geocomputation with R offers a more comprehensive explanation of available geometric, attribute and spatial data operations. For a quick overview, you may turn to the sf cheatsheets.\n\n\nIn order to test the code on your machine, download the North Carolina dataset and install the libraries sf and Rcpp before you run the code in an R-Script.\n\n\n\n\n\n\nExercise\n\n\n\nThe plot() function offers a large number of arguments that can be used to customize your map. Replace ‘AREA’ in the map above by a more meaningful map title. Turn to the documentation for more information.\nSee our solution!",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Read and Write Data</span>"
    ]
  },
  {
    "objectID": "08-read-write.html#read-and-write-raster-data",
    "href": "08-read-write.html#read-and-write-raster-data",
    "title": "8  Read and Write Data",
    "section": "\n8.3 Read and write raster data",
    "text": "8.3 Read and write raster data\nThe terra library provides functions to read and write raster data. In lesson Spatial Data Structures, we used the terra library to build SpatRaster Objects from scratch, investigated their properties and assigned and transformed coordinate reference systems.\nIt is more common, however, to create a SpatRaster Object from a file. Supported file formats for reading are GeoTIFF, ESRI, ENVI, and ERDAS. Most formats supported for reading can also be written to (see Creating SpatRaster objects).\nThe following code reads a GeoTIFF (download venice-Sentinel.tiff 712KB) into memory and assigns it to a variable named r:\n\nlibrary(terra)\n\nr &lt;- terra::rast(\"data/venice-Sentinel.tiff\")\n\nThe function hasValues returns the logic value TRUE, which indicates that in-memory layers have cell values:\n\nterra::hasValues(r)\n\n[1] TRUE\n\n\nExecuting the variable name r returns additional metadata information, which among others reveals that the image has 4 layers:\n\nr\n\nclass       : SpatRaster \ndimensions  : 503, 692, 4  (nrow, ncol, nlyr)\nresolution  : 7.025638, 6.675899  (x, y)\nextent      : 288674.2, 293535.9, 5033073, 5036431  (xmin, xmax, ymin, ymax)\ncoord. ref. : WGS 84 / UTM zone 33N (EPSG:32633) \nsource      : venice-Sentinel.tiff \nnames       : venice-Sentinel_1, venice-Sentinel_2, venice-Sentinel_3, venice-Sentinel_4 \n\n\nLayers 1-3 correspond to color bands red, green, and blue. In order to plot the true color sentinel imagery, we can use terra function plotRGB and assign the numbers of respective color bands to attributes r (red), g (greeb), b (blue):\n\nterra::plotRGB(r, r=1, g=2, b=3, main=\"Sentinel-2 image of Venice\")\n\n\n\n\n\n\n\nThe following line converts the original CRS (which is WGS 84 / UTM zone 33N) to the Italian national CRS Italy Zone 1:\n\nr_z1 &lt;- terra::project(r, \"+proj=tmerc +lat_0=0 +lon_0=9 +k=0.9996 +x_0=1500000 +y_0=0 +ellps=intl +units=m +no_defs\")\n\nEventually, the modified SpatRaster Object is written to file by means of terra function writeRaster:\n\nterra::writeRaster(r_z1, \"data/venice_zone1.tif\", overwrite=TRUE)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Read and Write Data</span>"
    ]
  },
  {
    "objectID": "08-read-write.html#data-api",
    "href": "08-read-write.html#data-api",
    "title": "8  Read and Write Data",
    "section": "\n8.4 Data API",
    "text": "8.4 Data API\n\nAPI is the acronym for Application Programming Interface, which is a software intermediary that allows two applications to talk to each other.\n\nBy means of an API we can read, write, and modify information on the web. The following video briefly introduces the technology behind it.\n\n\n\n\nThe most important takeaway messages are:\n\nWith a REST API, web data is accessible through a URL (Client-Server call via HTTP protocol)\nThe HTTP Get Method delivers data (a Response) - i.e. is used to read data, the HTTP Post Method is used to create new REST API resources (write data).\nURL Parameters are used to filter specific data from a response.\nTypically, APIs can return data in a number of different formats.\nJSON is a very popular format for transferring web data.\nThe two primary elements that make up JSON are keys and values.\n\nThe library httr2 offers functions to programmatically implement API calls in an R script. We will make use of this library to let our R script interact with the APIs offered by Geosphere Austria that contains historical weather data and weather forecasts in time series or gridded formats for retrieval. In the upcoming example we will make a call to the INCA Dataset, which offers hourly data on temperature, precipitation, wind, solar radiation, humidity and air pressure.\nFor accessing the data with httr2, we’ll construct a URL composed of a reference to the data source (base), and parameters to filter the desired data subset. To assemble base URL and parameters we will use the function sprintf().\n\nThe sprintf() function in R is used for string formatting. It allows you to combine, format, and interpolate variables into strings in a flexible way. For example, sprintf(\"%s?parameters=%s&start=%s\", base, lat, lon) constructs a URL string by inserting the values of base, lat and lon into the placeholders (%s). Each %s is replaced by the corresponding variable’s value in the order they are listed. This function is particularly useful in scenarios where you need to dynamically generate strings or URLs with variable data.\n\n\nlibrary(httr2)\n\n# Define the base URL and parameters\nbase &lt;- \"https://dataset.api.hub.geosphere.at/v1/timeseries/historical/inca-v1-1h-1km\"\nparameters &lt;- \"T2M\"\nstart_date &lt;- \"2023-06-01T22:00\"\nend_date &lt;- \"2023-06-01T22:00\"\nlatlon &lt;- \"47.81,13.03\"  \noutput_format &lt;- \"geojson\"\n\n# Construct the full URL with query parameters using sprintf\nfull_url &lt;- sprintf(\"%s?parameters=%s&start=%s&end=%s&lat_lon=%s&output_format=%s\", base, parameters, start_date, end_date, latlon, output_format)\n\n# Display the constructed URL\nprint(full_url)\n\n[1] \"https://dataset.api.hub.geosphere.at/v1/timeseries/historical/inca-v1-1h-1km?parameters=T2M&start=2023-06-01T22:00&end=2023-06-01T22:00&lat_lon=47.81,13.03&output_format=geojson\"\n\n\nThe base URL (base) in the code above specifies version (v1), type (timeseries), mode (historical) and resource-id (inca-v1-1h-1km) of the dataset being requested. This structure is described under Endpoint Structure in the API Documentation.\n\n\n\n\n\n\nTip\n\n\n\nAll Geosphere datasets as well as available types, modes and response formats can be listed via https://dataset.api.hub.geosphere.at/v1/datasets.\n\n\nAdditional metadata of the endpoint can be requested by appending /metadata to the base URL. For instance, https://dataset.api.hub.geosphere.at/v1/timeseries/historical/inca-v1-1h-1km/metadata returns a JSON that lists information on the resolution (hourly), temporal and spatial extent of the data as well as variables (parameters) and formats (output_format) that are available with this endpoint.\n\nBefore you assemble request URLs, it is recommended to test different calibrations in the FastAPI frontend.\n\nThe assembled request URL full_url (see code above) retrieves air temperature 2m above ground (T2M) for an individual point in time (2023-06-01T22:00) as geojson from the INCA dataset, which is a time series with hourly resolution.\nAs a next step, the URL is passed to the request() function to create a request object, and req_perform() is used to execute the HTTP Get method:\n\n# Create the request and perform it\nreq &lt;- httr2::request(full_url)\nresp &lt;- httr2::req_perform(req)\n\nBy default, the req_perform() function returns a response object. Printing a response object provides useful information such as the actual URL used (after any redirects), the HTTP status, and the content type.\nYou can extract important parts of the response with various helper functions such as resp_status() and resp_body_json():\n\n# Check the status code of the response\nhttr2::resp_status(resp)\n\n[1] 200\n\n# View the content structure of the response in JSON\nstr(httr2::resp_body_json(resp))\n\nList of 5\n $ media_type: chr \"application/json\"\n $ type      : chr \"FeatureCollection\"\n $ version   : chr \"v1\"\n $ timestamps:List of 1\n  ..$ : chr \"2023-06-01T22:00+00:00\"\n $ features  :List of 1\n  ..$ :List of 3\n  .. ..$ type      : chr \"Feature\"\n  .. ..$ geometry  :List of 2\n  .. .. ..$ type       : chr \"Point\"\n  .. .. ..$ coordinates:List of 2\n  .. .. .. ..$ : num 13\n  .. .. .. ..$ : num 47.8\n  .. ..$ properties:List of 1\n  .. .. ..$ parameters:List of 1\n  .. .. .. ..$ T2M:List of 3\n  .. .. .. .. ..$ name: chr \"air temperature\"\n  .. .. .. .. ..$ unit: chr \"degree_Celsius\"\n  .. .. .. .. ..$ data:List of 1\n  .. .. .. .. .. ..$ : num 16.3\n\n\nTo facilitate subsequent analyses and data visualization, we can convert the content of the return object to a data frame. With httr2, you can directly retrieve the JSON content and convert it to a data frame:\n\nlibrary(knitr)\n\n# Using resp_body_json to get the JSON content of the response\nresponse_content &lt;- httr2::resp_body_json(resp)\nresponse_df &lt;- as.data.frame(response_content)\n\n# Select columns 7, 8, 10 and 11 of response data frame \n# rename columns and render data frame as html table\nnames &lt;- c(\"X\", \"Y\", \"unit\", \"temp\")\nresponse_df %&gt;%\n  dplyr::select(7, 8, 10, 11) %&gt;%\n  setNames(., names) %&gt;%\n  knitr::kable(., format=\"html\")\n\n\n\nX\nY\nunit\ntemp\n\n\n13.02537\n47.81396\ndegree_Celsius\n16.29\n\n\n\n\nFor purposes of presentation, the returned data frame was shortened to show only four columns.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Read and Write Data</span>"
    ]
  },
  {
    "objectID": "09-spatial-data-manipulation.html",
    "href": "09-spatial-data-manipulation.html",
    "title": "9  Spatial Data Manipulation",
    "section": "",
    "text": "9.1 Data Acquisition\nIn the previous lesson, you learned how to access data on a server via a REST API. The Open Geospatial Consortium (OGC) has adapted the REST API paradigm for geospatial applications. A variety of OGC API standards have been implemented to provide and utilize geospatial data on the web. An overview of the current implementation status can be found here.\nIn this lesson, we will utilize online data resources in workflows that involve data cleaning, spatial queries, and analyses. Additionally, you will learn to carry out basic raster manipulation operations on a sample raster dataset.\nWe will work with vector data to correctly identify agricultural land parcels in European Union countries. The Austrian Agricultural Agency (AMA) provides access to Austrian agricultural parcels through an OGC Rest API - Feature interface. The OGC API Features standard is the successor to the OGC Web Feature Service (WFS) specification.\nBefore loading the data into an R script, examine the API’s contents. Visit the web service’s landing page and proceed to the collection page. There you will find an overview of the available layers.\nEnter the following URL into your browser:\nThis request returns the first ten features of the i009501:invekos_feldstuecke_2024_1 layer (agricultural land parcels as polygons) in GeoJSON format.\nUpon inspecting the GeoJSON, you will find the coordinate vertices of the polygon features and attributes such as fs_flaeche_ha (parcel area in hectares) and fnar_bezeichnung (land use).\nUse the bbox parameter to filter resources within a specific area:\nFor a visual representation of the bounding box, enter the coordinates 14,48,14.02,48.02 into linestrings.com and click “Display box”.\nTo execute the request in R, use the following script:\nlibrary(httr2)\nlibrary(geojsonsf)\nlibrary(sf)\n\n# Define the new API endpoint\nfull_url &lt;- \"https://gis.lfrz.gv.at/api/geodata/i009501/ogc/features/v1/collections/i009501:invekos_feldstuecke_2024_1/items?f=json&bbox=14,48,14.02,48.02\"\n\n# Make the request and convert the response to an sf object\ninvekos &lt;- request(full_url) %&gt;%   # Create request\n  req_perform() %&gt;%                # Execute request\n  resp_body_string() %&gt;%           # Extract JSON body as string\n  geojsonsf::geojson_sf()          # Convert JSON string to sf object\nThe above code retrieves polygon features within the specified bounding box and stores them in an object named invekos.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Spatial Data Manipulation</span>"
    ]
  },
  {
    "objectID": "09-spatial-data-manipulation.html#data-acquisition",
    "href": "09-spatial-data-manipulation.html#data-acquisition",
    "title": "9  Spatial Data Manipulation",
    "section": "",
    "text": "Tip\n\n\n\nThe R syntax used to interact with OGC APIs is the same as described in the “Data API” section of Lesson Read and Write Data.\n\n\n\n\nhttps://gis.lfrz.gv.at/api/geodata/i009501/ogc/features/v1/collections/i009501:invekos_feldstuecke_2024_1/items?f=json&limit=10\n\n\nGeoJSON is a JSON-based standard for representing simple geographic features, along with their non-spatial attributes.\n\n\n\nhttps://gis.lfrz.gv.at/api/geodata/i009501/ogc/features/v1/collections/i009501:invekos_feldstuecke_2024_1/items?f=json&bbox=14,48,14.02,48.02",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Spatial Data Manipulation</span>"
    ]
  },
  {
    "objectID": "09-spatial-data-manipulation.html#data-cleaning",
    "href": "09-spatial-data-manipulation.html#data-cleaning",
    "title": "9  Spatial Data Manipulation",
    "section": "\n9.2 Data Cleaning",
    "text": "9.2 Data Cleaning\nOnce the data is loaded into R, we can more closely examine its structure. The dplyr function glimpse, for instance, allows us to preview each column’s name and type:\n\nlibrary(dplyr)\ndplyr::glimpse(invekos)\n\nRows: 100\nColumns: 15\n$ fart_id           &lt;dbl&gt; 1822, 1822, 1822, 1822, 1822, 1822, 1822, 1822, 1822…\n$ gml_geom          &lt;chr&gt; \"[B@1a17f016\", \"[B@38eebec6\", \"[B@64b28af0\", \"[B@76d…\n$ gml_length        &lt;dbl&gt; 1086, 883, 786, 1599, 482, 718, 1432, 1094, 884, 116…\n$ geom_date_created &lt;chr&gt; \"2022-10-01T05:56:50+02:00\", \"2022-10-01T05:56:50+02…\n$ log_pkey          &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3…\n$ fnar_code         &lt;chr&gt; \"A\", \"A\", \"G\", \"A\", \"G\", \"A\", \"A\", \"A\", \"A\", \"A\", \"G…\n$ gml_identifier    &lt;chr&gt; \"https://data.inspire.gv.at/0095/33078dd6-05e0-4263-…\n$ fs_kennung        &lt;dbl&gt; 111714134, 111714135, 111714132, 111793656, 11179365…\n$ geo_part_key      &lt;dbl&gt; 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, …\n$ gml_id            &lt;chr&gt; \"AT.0095.33078dd6-05e0-4263-a464-f35bd1607ecd.elu.Ex…\n$ inspire_id        &lt;chr&gt; \"https://data.inspire.gv.at/0095/33078dd6-05e0-4263-…\n$ fnar_bezeichnung  &lt;chr&gt; \"ACKERLAND\", \"ACKERLAND\", \"GRÜNLAND\", \"ACKERLAND\", \"…\n$ fs_flaeche_ha     &lt;dbl&gt; 1.52789444, 1.76240452, 0.20852359, 7.88961123, 0.02…\n$ geo_id            &lt;dbl&gt; 12316762, 12316764, 12316768, 12301276, 12301279, 12…\n$ geometry          &lt;POLYGON [°]&gt; POLYGON ((14.01608 48.00954..., POLYGON ((14…\n\n\n\nThe abbreviation dbl stands for double, a data type that stores numbers with decimal points.\n\nTo create a subset of the invekos sf-object, use the following code:\n\ninvekos %&gt;%\n  head(2) %&gt;%\n  dplyr::select(fnar_bezeichnung, fs_flaeche_ha, geo_id, geometry) %&gt;%\n  knitr::kable(., format=\"html\")\n\n\n\nfnar_bezeichnung\nfs_flaeche_ha\ngeo_id\ngeometry\n\n\n\nACKERLAND\n1.527894\n12316762\nPOLYGON ((14.01608 48.00954...\n\n\nACKERLAND\n1.762404\n12316764\nPOLYGON ((14.01608 48.00931...\n\n\n\n\n\nField names and entries are in German. To rename them, use the base R function colnames():\n\n# Subset of invekos object\ninvekos.sub &lt;- invekos %&gt;%\n  dplyr::select(fnar_bezeichnung, fs_flaeche_ha, geo_id, geometry)\n\n# Renaming fields\ncolnames(invekos.sub)[1] &lt;- \"land_use\"\ncolnames(invekos.sub)[2] &lt;- \"area_ha\"\n\nEntries are renamed using dplyr’s mutate and case_when functions:\n\nunique(invekos.sub$land_use)  # Unique entries in 'land_use'\n\n[1] \"ACKERLAND\" \"GRÜNLAND\" \n\n# Renaming entries 'ACKERLAND' and 'GRÜNLAND'\ninvekos.sub &lt;- dplyr::mutate(invekos.sub, land_use = \n                               case_when(land_use == 'ACKERLAND' ~ 'arable land',\n                               land_use == 'GRÜNLAND' ~ 'grassland', TRUE ~ 'Other'))\n\n# Display result\ninvekos.sub %&gt;%\n  head(2) %&gt;%\n  knitr::kable(., format=\"html\")\n\n\n\nland_use\narea_ha\ngeo_id\ngeometry\n\n\n\narable land\n1.527894\n12316762\nPOLYGON ((14.01608 48.00954...\n\n\narable land\n1.762404\n12316764\nPOLYGON ((14.01608 48.00931...\n\n\n\n\n\nFor an initial visual impression, plot the sf-object using the base R plot() function:\n\nplot(invekos.sub[1], main=\"Land Use\", key.pos = 1, key.width = lcm(1.8))\n\n\n\n\n\n\n\nBy default, the plot() function generates a multi-panel plot, with one sub-plot for each field of the object. However, specifying invekos.sub[1] restricts the output to a single plot, specifically showcasing the land_use field. The function parameter key.pos = 1 aligns the legend below the map (1=below, 2=left, 3=above and 4=right). key.width defines the width of the legend.\nNext, we can validate the geometry of sf polygons using st_is_valid:\n\nsf::st_is_valid(invekos) %&gt;%\n  summary()\n\n   Mode    TRUE \nlogical     100 \n\n\nThe st_is_valid function checks the validity of each geometry, returning a logical vector. The summary confirms that all geometries in our dataset are valid. If any invalid geometries are detected, they can be corrected using the st_make_valid function.\n\n\n\n\n\n\nExercise\n\n\n\nLet’s closely investigate a single polygon geometry feature that is created by the code below. The validity check flags an invalid geometry. Can you identify the problem?\n\n\n\n# Coordinates for the polygon\ncoords &lt;- matrix(c(-1, -1, 1, -1, 1, 1, 0, -1, -1, -1), ncol = 2, byrow = TRUE)\n\n# List of coordinate matrices\nlist_of_coords &lt;- list(coords)\n\n# Constructing the polygon\npolygon &lt;- st_polygon(list_of_coords)\n\n# Converting to an sf object\nerror_sf &lt;- st_sf(geometry = st_sfc(polygon))\n\n# Validity check\nsf::st_is_valid(error_sf)\n\n[1] FALSE\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWhen plotted, the polygon reveals a sliver polygon, which is invalid due to its shape.\n\nplot(error_sf)\n\n\n\n\n\n\n\nAnother common cause of invalid geometries is self-intersection of lines.\n\nNote: A valid polygon requires at least four coordinate points with the first and last points being identical. A polygon with only three points is considered to have an invalid geometry.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Spatial Data Manipulation</span>"
    ]
  },
  {
    "objectID": "09-spatial-data-manipulation.html#vector-operations",
    "href": "09-spatial-data-manipulation.html#vector-operations",
    "title": "9  Spatial Data Manipulation",
    "section": "\n9.3 Vector operations",
    "text": "9.3 Vector operations\nThe structure of an sf object is similar to that of data frames, enabling attribute-based filtering operations using dplyr functions, as described in Lesson Data Manipulation.\nFor example, to extract grassland parcels, the filter() function is used:\n\ninvekos.sub %&gt;%\n  dplyr::filter(land_use=='grassland') %&gt;%\n  {plot(.[1], main=\"Land Use\", key.pos = 1, key.width = lcm(1.8))}\n\n\n\n\n\n\n\n\nNote the use of curly brackets in the plot() function. This is required because the dot notation (.) cannot directly index a piped value. For a deeper explanation, see this Stack Overflow discussion.\n\n\n9.3.1 Geometrical operations\nThe sf package provides a range of geometric predicates such as st_within, st_contains, or st_crosses. A complete list is available here. While these functions are often used between pairs of simple feature geometry sets, they can also operate on a single sf object:\n\nsf::st_intersects(invekos.sub, sparse = TRUE)\n\nSparse geometry binary predicate list of length 100, where the\npredicate was `intersects'\nfirst 10 elements:\n 1: 1\n 2: 2\n 3: 3, 68\n 4: 4, 38, 42\n 5: 5, 71\n 6: 6, 58, 64, 78\n 7: 7, 39\n 8: 8, 33, 75, 77\n 9: 9, 12, 15, 66\n 10: 10\n\n\nThis function returns a sparse matrix revealing intersections within features of the same sf object invekos.sub, indicating possible topology errors. For correcting these errors, GIS software like QGIS is recommended as R’s capabilities for handling topology errors are less advanced.\n\n9.3.2 Binary operations\nBinary operations are essential for analyzing spatial relationships between different datasets. Here, we demonstrate this by creating an sf object representing farmsteads using data from the AMA Rest API service:\n\n# Define the new API endpoint for farmsteads\nfarms_url &lt;- \"https://gis.lfrz.gv.at/api/geodata/i009501/ogc/features/v1/collections/i009501:invekos_hofstellen_2024_1/items?f=json&bbox=14,48,14.02,48.02&limit=100\"\n\n# Make the request and convert the response to an sf object\nfarms &lt;- httr2::request(farms_url) %&gt;%   # Create request\n  httr2::req_perform() %&gt;%               # Execute request\n  httr2::resp_body_string() %&gt;%          # Extract JSON body as string\n  geojsonsf::geojson_sf()                # Convert JSON string to sf object\n\n# Plot land parcels with farm points overlayed\nplot(invekos.sub[1], main = NULL, key.pos = NULL, reset = FALSE, col_graticule = \"grey\")\nplot(farms[1], main = NULL, key.pos = NULL, pch = 7, col = 'red', add = TRUE, cex = 1)\n\n\n\n\n\n\n\n\nNote: The plot() function is used sequentially to overlay farm points on top of the land parcel polygons. For advanced plotting techniques, refer to the plot() documentation.\n\nWe proceed to calculate the distances between farms and land parcels to determine proximity:\n\ndist_m &lt;- sf::st_distance(farms, invekos.sub)\n\nThe st_distance function calculates the shortest distance matrix between the geometries of the two sf objects, with distances returned in meters.\n\n\n\n\n\n\nExercise\n\n\n\nDistances are computed in meters, which may seem unexpected since the reference system’s units are in degrees. To understand the underlying calculations, examine the coordinate reference system of the sf objects farms and invekos.sub using sf::st_crs(). This will reveal that objects use a projected coordinate system (EPSG:31287), which allows for metric distance calculations.\nIn this short exercise, we will take a closer look at the algorithm that is implemented in the function st_distance.\nOpen the documentation of st_distance to find out how metric distances are derived from geographic coordinates.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAccording to the documentation, metric distances are computed based on the Coordinate Reference System (CRS) of the sf objects. Since both farms and invekos.sub use EPSG:31287 (a projected CRS), the st_distance function calculates planar (Euclidean) distances in meters. This is possible because projected CRSs preserve distance relationships locally, unlike geographic CRSs which use degrees.\nFor more detailed information on distance calculations, refer to Algorithms for geodesics, Journal of Geodesy.\n\n\n\nWhen plotting the complete distance matrix dist_m, we see that column 1 contains the distances between the first feature (parcel 1) in the invekos.sub sf object and the 100 farm features of the farms sf object. Accordingly, the matrix has 100 columns (one for every parcel) and 100 rows (one for every farm).\nThe following line returns the first column of the distance matrix as a vector:\n\ndist_m[, 1]\n\nUnits: [m]\n [1]  143.1406  813.3666  825.8695  211.4067 1360.1150  964.9589  804.7171\n [8] 1025.2758  204.4024  810.4991  898.8973\n\n\nTo identify the farm that is located closest to parcel 1, we need to query the index of the minimum value in this vector:\n\nwhich(dist_m[, 1] == min(dist_m[, 1]))\n\n[1] 1\n\n\nThe which function, a base R utility, identifies the indices of elements that satisfy a given condition. In the example, it returns the index of the smallest value within the vector dist_m[, 1]. Consequently, it indicates which farm is the closest to parcel 1.\nThe demonstrated procedure for detecting closest farms can be executed for every parcel in a for-loop. The number of the closest farm is appended to a vector named closest. This vector is in turn appended as a new column to the sf object invekos.sub. Finally, invekos.sub is plotted together with farms.\nThere are now two approaches to perform this loop:\nApproach 1: Dynamic Loop Using ncol(dist_m)\nThis method dynamically adjusts the loop to the number of parcels by using ncol(dist_m). It’s flexible and scalable, making it suitable for datasets where the number of parcels may change.\n\n# Approach 1: Dynamic Loop Using ncol(dist_m)\nclosest &lt;- c()\n\nfor (i in 1:ncol(dist_m)){\n  \n  out &lt;- which(dist_m[, i] == min(dist_m[, i]))\n  \n  closest &lt;- c(closest, out)\n  \n}\n\ncbind(invekos.sub, closest) %&gt;%\n  {plot(.[4], main = NULL, key.pos = NULL, reset = FALSE)}\n\nplot(farms[1], main = NULL, key.pos = NULL, pch = 7, col = 'red', add = TRUE, cex = 1)\n\n\n\n\n\n\n\nApproach 2: Fixed Loop Using a Hardcoded Value (e.g., 100)\nThis method uses a fixed number of iterations (100) based on prior knowledge of the dataset size. It’s simpler but less flexible, as it requires manual adjustment if the number of parcels changes.\n\n# Approach 2: Fixed Loop Using a Hardcoded Value (e.g., 100)\nclosest &lt;- c()\n\nfor (i in 1:100){\n  \n  out &lt;- which(dist_m[, i] == min(dist_m[, i]))\n  \n  closest &lt;- c(closest, out)\n  \n}\n\ncbind(invekos.sub, closest) %&gt;%\n  {plot(.[4], main = NULL, key.pos = NULL, reset = FALSE)}\n\nplot(farms[1], main = NULL, key.pos = NULL, pch = 7, col = 'red', add = TRUE, cex = 1)\n\n\n\n\n\n\n\nThe output map visualizes the closest farm to each agricultural parcel, highlighting the practical application of sf Geometrical operations.\n\n\n\n\n\n\nTip\n\n\n\nThis chapter focuses on logical matrix outputs from geometric operations. For operations generating new geometries, such as st_union, st_buffer, and st_centroid, see operations returning a geometry. For network operations on sf objects, consider using sfnetworks or the igraph package.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Spatial Data Manipulation</span>"
    ]
  },
  {
    "objectID": "09-spatial-data-manipulation.html#raster-operations",
    "href": "09-spatial-data-manipulation.html#raster-operations",
    "title": "9  Spatial Data Manipulation",
    "section": "\n9.4 Raster operations",
    "text": "9.4 Raster operations\nHaving previously discussed the structure of SpatRaster Objects in Lesson 4s and the reading and writing of such objects in Lesson 8, we now turn our attention to raster manipulation operations like resampling and cropping.\n\nFor the following examples, we will utilize a sample dataset from the terra package. You can download the sample data here.\n\n\n9.4.1 Resampling\nResampling is crucial for working with raster datasets. It alters the spatial resolution, allowing you to align multiple rasters with different resolutions. Additionally, it adjusts the level of detail necessary for your analysis.\nLet’s demonstrate resampling with the terra sample dataset:\n\nlibrary(terra)\n\nr &lt;- terra::rast(\"data/terra_sample.tif\") # path may be different on your machine\nplot(r, main='SpatRaster from file')\n\n\n\n\n\n\n\nBefore we change the raster resolution of SpatRaster Object, it is important to know the original resolution of the raster. You can use the res() function to check the original resolution:\n\nres(r)\n\n[1] 40 40\n\n\nFor resampling, we’ll create a target raster by copying the original and setting a new resolution:\n\nr2 &lt;- r\nterra::res(r2) &lt;- 80 # Assigning a new resolution of 80x80\n\nAlthough this operation clears the raster values, r2 can still be used as a target for resampling:\n\nr_resampled &lt;- terra::resample(r, r2, method=\"bilinear\")\n\nres(r2) # Verify the new resolution\n\n[1] 80 80\n\nplot(r_resampled, main='Resampled 80x80')\n\n\n\n\n\n\n\nThe bilinear method is used for interpolating new values in r_resampled. Alternative methods are nearest or cubic. The choice of interpolation method for resampling raster data is a crucial consideration, discussed in further detail here.\n\n9.4.2 Crop raster\nRaster cropping allows you to select a specific area from a larger raster dataset for targeted analysis. Both SpatRaster and SpatExtent objects can be utilized for cropping operations.\nTo derive an extent from a SpatRaster, the ext function is used:\n\nterra::ext(r)\n\nSpatExtent : 178400, 181600, 329400, 334000 (xmin, xmax, ymin, ymax)\n\n\nThe output format is &lt;xmin, xmax, ymin, ymax&gt;. Knowing this, we can define a cropping extent and apply it using the crop function:\n\ncrop_ext &lt;- terra::ext(180000, 180200, 331000, 331250)\nsubset_r &lt;- terra::crop(r, crop_ext)\nplot(subset_r, main='Subset of r')\n\n\n\n\n\n\n\nIf the resulting cropped area doesn’t match expectations due to cell alignment, adjust the cropping extent or raster resolution as necessary.\n\n9.4.3 Raster algebra\nTerra provides functionality for algebraic operations on rasters, supporting standard arithmetic and logical operators, and functions like abs and round. For a full list, visit the terra algebra documentation.\nHere’s how you can perform some simple operations:\n\n# Add a constant value to each cell\ns &lt;- r + 10\n# Calculate the square root of cell values\ns &lt;- sqrt(r)\n# Generate a logical raster based on cell value equality\ns1 &lt;- s == 15\n\nplot(s1)\n\n\n\n\n\n\n\nOperators can be applied between SpatRaster objects with matching resolutions and origins. The result covers their spatial intersection.\nYou can also perform in-place replacements:\n\nr[r == 255] &lt;- 2550\nplot(r)\n\n\n\n\n\n\n\nHere, all cells with the value 255 are replaced with 2550.\n\n\n\n\n\n\nTip\n\n\n\nA broader range of vector and raster operations can be found in Chapter 5 - Geometry Operations, in the Book Geocomputation with R.\nAs of writing this module, “Geocomputation with R” uses the raster package, which is expected to be superseded by terra.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Spatial Data Manipulation</span>"
    ]
  },
  {
    "objectID": "10-data-viz.html",
    "href": "10-data-viz.html",
    "title": "10  Data Visualization",
    "section": "",
    "text": "10.1 The Grammar of Graphics\nR has a very rich set of graphical functions. The R Graph Gallery provides a large number of examples (including code).\nIn this lesson you will get to know the ggplot2 library, which is the most popular library for creating graphics in R. You will learn to create standard graphs such as histograms, boxplots or scatterplots as well as maps.\nThe ggplot2 library, a part of the Tidyverse suite, is renowned for its comprehensive and intuitive approach to data visualization in R. Rooted in the principles of “The Grammar of Graphics”, conceptualized by Leland Wilkinson, ggplot2 enables users to construct graphics through a layered approach, incorporating seven distinct elements.\nThe Grammar of Graphics is a schema that enables us to concisely describe the components of a graphic. These components are called layers of grammatical elements. Overall, the grammar comprises seven layers:\nIn essence, these layers enable a structured and flexible approach to crafting visual narratives from data.\nFor instance, the visual variables such as size, shape, and color offer nuanced ways to represent and differentiate data points. In the example below, ‘GPD per capita’ and ‘Life Expectancy’ align with the x and y axes, respectively, and ‘national population’ and ‘world regions’ are differentiated by size and color.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "10-data-viz.html#the-grammar-of-graphics",
    "href": "10-data-viz.html#the-grammar-of-graphics",
    "title": "10  Data Visualization",
    "section": "",
    "text": "Data: The core dataset to be visualized.\n\nAesthetics: Mappings of variables to visual scales, like color or size.\n\nGeometries: The visual representation of data, such as points, lines, or bars.\n\nFacets: Creating subsets of data to generate similar graphs for each subset.\n\nStatistics: Applying statistical transformations to data (mean, median, etc.).\n\nCoordinates: Managing axes and spatial transformation.\n\nThemes: Customizing the graphical backdrop for enhanced visual appeal.\n\n\n\n\n\n\n\n\n\nFigure 10.1: Visual variables in Bubble Chart. Source: Gapminder\n\n\n\n\n\n\n\n\nExercise\n\n\n\nIn the interactive window above, click Options. You will find settings for specifying visual variables like X and Y, Size and Color.\nThe Gapminder Project makes data from various sources like UN, FAO or World Bank accessible. You may select from those databases and assign other variables to X and Y, size or color.\nIn the upcoming examples, we will use ggplot2 together with function aes() to programmatically set mappings of visual variables in diagrams and maps.\n\n\n\n\n\n\n\n\nTip\n\n\n\nFor a foundational understanding of “The Grammar of Graphics” as implemented in ggplot2, refer to Hadley Wickham’s insightful article.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "10-data-viz.html#understanding-wide-and-long-data-formats-in-r",
    "href": "10-data-viz.html#understanding-wide-and-long-data-formats-in-r",
    "title": "10  Data Visualization",
    "section": "\n10.2 Understanding Wide and Long Data Formats in R",
    "text": "10.2 Understanding Wide and Long Data Formats in R\nWhen working with data in R, especially for visualization purposes, it’s crucial to understand the structure of your dataset. Typically, datasets can be categorized into two main formats: Wide and Long (also known as Tidy) formats. The format of your data can significantly impact how you manipulate and visualize it.\n\n10.2.1 Wide format\n\nIn a wide-format dataset, each subject or entity (such as a city) is represented once, with multiple columns for different variables or time periods.\n\nFor instance, consider a dataset representing annual rainfall measurements across different cities, measured in millimeters (mm). In this wide format, each row corresponds to a city, and the columns represent rainfall measurements for different years:\n\n# Example of a wide format dataset\nwide_data &lt;- data.frame(\n  City = c(\"CityA\", \"CityB\", \"CityC\", \"CityD\", \"CityE\"),\n  Rainfall_2015 = c(600, 500, 550, 450, 400),\n  Rainfall_2016 = c(650, 550, 600, 500, 450)\n)\n\nknitr::kable(wide_data)\n\n\n\nCity\nRainfall_2015\nRainfall_2016\n\n\n\nCityA\n600\n650\n\n\nCityB\n500\n550\n\n\nCityC\n550\n600\n\n\nCityD\n450\n500\n\n\nCityE\n400\n450\n\n\n\n\n\n\n10.2.2 Long format:\n\nIn a long-format dataset, each row is a single observation for a single variable, often requiring multiple rows per subject or unit.\n\nUsing the same rainfall dataset as an example, a long format would list each year’s rainfall for each city as a separate row:\n\n# Example of long-format data:\nlong_data &lt;- tidyr::pivot_longer( \n  wide_data,\n  cols = -City, \n  names_to = \"Year\", \n  values_to = \"Rainfall\"\n)\nknitr::kable(long_data)\n\n\n\nCity\nYear\nRainfall\n\n\n\nCityA\nRainfall_2015\n600\n\n\nCityA\nRainfall_2016\n650\n\n\nCityB\nRainfall_2015\n500\n\n\nCityB\nRainfall_2016\n550\n\n\nCityC\nRainfall_2015\n550\n\n\nCityC\nRainfall_2016\n600\n\n\nCityD\nRainfall_2015\n450\n\n\nCityD\nRainfall_2016\n500\n\n\nCityE\nRainfall_2015\n400\n\n\nCityE\nRainfall_2016\n450\n\n\n\n\n\nUnderstanding these formats is crucial for effective data manipulation and visualization in R, especially when using libraries like ggplot2 and tidyverse. Certain types of visualizations and statistical analyses are more straightforward with data in a specific format.\n\n\n\n\n\n\nTip\n\n\n\nTransforming data between wide and long formats is often achieved using functions like pivot_longer() and pivot_wider() from the tidyr package, which is included in the Tidyverse.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "10-data-viz.html#visualization-of-distributions",
    "href": "10-data-viz.html#visualization-of-distributions",
    "title": "10  Data Visualization",
    "section": "\n10.3 Visualization of distributions",
    "text": "10.3 Visualization of distributions\nAs already mentioned above, functions in the ggplot2 library are structured according to “The Grammar of Graphics”. When creating graphs with ggplot2, we start by setting up data and aesthetics (aes()), then defining the type of plot (geometry) like geom_point, and finally enhancing the plot with additional transformations and themes.\nWe begin the analysis with a simple histogram, to explore the distribution of air quality data that has been measured at different locations in Upper Austria.\nThe dataset contains the following variables:\n\n\nTime: Timestamp of each measurement.\n\nStation: Identifier for the measurement station.\n\nComponent: The air quality component measured.\n\nMeantype: The temporal resolution of measurements.\n\nUnit: The unit of measurement.\n\nValue: The measured value of air quality.\n\nLet’s start by displaying the first few rows of this dataset:\n\nlibrary(tidyverse)\nlibrary(knitr)\n\n# Read the dataset (Note: Semi-colon separated)\nairquality &lt;- read_delim(\"data/AirQualityUpperAut.csv\", delim = \";\")\n\n# Display the first five rows\nairquality %&gt;%\n  dplyr::slice_head(n = 5) %&gt;%\n  knitr::kable()\n\n\n\ntime\nstation\ncomponent\nmeantype\nunit\nvalue\n\n\n\n21.10.2021 13:30\nC001\nBOE\nHMW\nm/s\n14.1\n\n\n21.10.2021 14:00\nC001\nBOE\nHMW\nm/s\n12.0\n\n\n21.10.2021 14:30\nC001\nBOE\nHMW\nm/s\n10.1\n\n\n21.10.2021 15:00\nC001\nBOE\nHMW\nm/s\n7.9\n\n\n21.10.2021 15:30\nC001\nBOE\nHMW\nm/s\n9.2\n\n\n\n\n\nThe code below filters the airquality dataset by measurement component and temporal resolution. Then the data subset is passed as a first argument to function ggplot(). In the second argument, we map the variable value onto the x-axis with the aesthetics argument aes(). geom_histogram() specifies the geometry of the plot and theme_bw() is used to add a background theme.\n\n# filter NO2 measurements with temporal resolution 30min (HMW)\nairquality %&gt;%\n  dplyr::filter(component == \"NO2\" & meantype == \"HMW\") %&gt;%\n\n  # create plot  \n  ggplot2::ggplot(.,    # the dot '.' represents the piped value \n    aes(            \n      x = value         # map variable 'value' onto x-axis\n    )\n  ) +\n  ggplot2::geom_histogram() +    # define geometry\n  ggplot2::theme_bw()            # define theme\n\n\n\n\n\n\n\nTo differentiate between measurements from various stations, we can map the station variable to the color attribute:\n\nairquality %&gt;%\n  dplyr::filter(component == \"NO2\" & meantype == \"HMW\") %&gt;%\n  dplyr::filter(station == \"S125\" | station == \"S431\" | station == \"S270\") %&gt;%    # select 3 stations\n\n  ggplot2::ggplot(.,    \n    aes(            \n      x = value, \n      fill = station\n      \n    )\n  ) +\n  \n  ggplot2::xlab(\"NO2 [mg/m^3]\") + # add x-axis label\n  ggplot2::ylab(\"Count\") +        # add y-axis label\n  scale_fill_manual(name = \"Measurement stations\", values = c(\"grey20\", \"grey50\", \"grey80\")) + # add legend\n  ggplot2::geom_histogram() +   \n  ggplot2::theme_bw()            \n\n\n\n\n\n\n\nThis is implemented by adding an attribute fill = station to the aesthetics element (aes()). ggplot2 offers a number of functions to specify your own set of mappings from levels in the data to aesthetic values. In the example above the function scale_fill_manual() is used to map the three levels S125, S270 and S431 to the fill colors grey20, grey50 and grey80. Instead of ‘ggplot colors’, you can also use hex color codes.\n\nNote that plot components are added by means of a plus ‘+’ sign. It allows you to start simple, and then get more and more complex.\n\n\n\n\n\n\n\nExercise\n\n\n\nSo far, we have added two axis labels. Create a new R-Script, download the input data, recreate the histogram and insert one additional line of code to add a plot title (see documentation).\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nInsert a title by adding:\n\nggplot2::ggtitle(\"Nitrogen dioxide concentration\")",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "10-data-viz.html#boxplots",
    "href": "10-data-viz.html#boxplots",
    "title": "10  Data Visualization",
    "section": "\n10.4 Boxplots",
    "text": "10.4 Boxplots\nThe same basic syntax is used to create other types of plots like bar plots (use geometry geom_bar() or geom_col(), line plots (use geometry geom_line()) and many others.\nFor instance, if we replace geom_histogram() by geom_boxplot(), the value distribution of NO2 measurements is visualized by means of a box plot:\n\n# filter NO2 measurements with temporal resolution 30min (HMW)\nairquality %&gt;%\n  dplyr::filter(component == \"NO2\" & meantype == \"HMW\") %&gt;%\n\n  # create plot  \n  ggplot2::ggplot(.,    # the dot '.' represents the piped value \n    aes(            \n      x = value         # map variable 'value' onto x-axis\n    )\n  ) +\n  ggplot2::xlab(\"NO2 [mg/m^3]\") +\n  ggplot2::geom_boxplot() +    # define geometry\n  ggplot2::theme(\n    axis.text.y = element_blank(),    # remove text and ticks from y axis\n    axis.ticks.y = element_blank()\n  )\n\n\n\n\n\n\n\n\nNote that the last two lines remove text and tick marks from the y-axis of the plot.\n\nJust as histograms, box plots are used to inspect distributions in data. The interpretation, however, does require some additional information.\nThe lower and upper edge of the box (the so-called lower and upper hinges) correspond to the first and third quartiles. The vertical line that separates the box indicates the median value (second quartile).\nThe upper whisker extends from the hinge to the largest value no further than 1.5 * IQR from the hinge (where IQR is the inter-quartile range, or distance between the first and third quartiles). The lower whisker extends from the hinge to the smallest value at most 1.5 * IQR of the hinge. Data beyond the end of the whiskers are called “outlying” points and are plotted individually.\n\n\n\n\n\n\nExercise\n\n\n\nIn our histogram examples, we have mapped the variable ‘station’ onto visual variable color to separately visualize measurements of different stations. Try to apply the same approach to render measurements of stations S125, S270 and S431 separately in a box plot.\nSee our solution!",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "10-data-viz.html#scatterplots",
    "href": "10-data-viz.html#scatterplots",
    "title": "10  Data Visualization",
    "section": "\n10.5 Scatterplots",
    "text": "10.5 Scatterplots\nScatterplots serve as a pivotal tool in statistical analysis, particularly when it comes to exploring the interplay between two variables. Their strength lies in visually capturing the nature and strength of relationships.\nConsider the scenario where we seek to understand the relationship between air temperature (TEMP) and relative humidity (RF). For this purpose, we delve into the airquality dataset, focusing on half-hourly readings from station S108. Our objective is to juxtapose the temperature and humidity readings and analyze their correlation:\n\n# Extract half-hourly temperature readings from station S108\ntemp_tab &lt;- airquality %&gt;%\n  dplyr::filter(component == \"TEMP\", meantype == \"HMW\", station == \"S108\") \n\n# Similarly, extract half-hourly humidity readings\nhumidity_tab &lt;- airquality %&gt;%\n  dplyr::filter(component == \"RF\", meantype == \"HMW\", station == \"S108\") \n\n# Join the two datasets on 'time'\ntemp_humidity_joined &lt;- temp_tab %&gt;%\n  dplyr::inner_join(humidity_tab, by = \"time\") %&gt;%\n  dplyr::select(time, value.x, value.y) \n\n# Create the scatterplot\nggplot(temp_humidity_joined, aes(x = value.x, y = value.y)) +\n  xlab(\"Air Temperature [°C]\") +\n  ylab(\"Relative Humidity [%]\") +\n  geom_point(color = \"blue\") +\n  geom_smooth(method = lm, color = \"red\", fill = \"#69b3a2\", se = TRUE) +\n  theme_minimal()          \n\n\n\n\n\n\n\nThe generated scatterplot vividly illustrates the relationship between air temperature and relative humidity. Typically, as the temperature drops, the relative humidity tends to rise, and vice versa. This inverse relationship, although not strictly linear, can be approximated using a linear regression model. Deviations from this model are captured within a 95% confidence interval, providing a more nuanced understanding of the data dynamics.\nFor a comprehensive explanation of the inverse relationship between relative humidity and temperature, refer to this video\n\n\n\n\n\n\nExercise\n\n\n\nCopy and run the code example from above in a new R-Script. Note that the air quality data as well as the tidyverse library must be loaded to run the code in a standalone R-script file.\nComplete Script!\nWhile exploring, consider the following:\n\nHow many individual measurements are visualized in the scatterplot?\nWhat do value.x and value.y represent in this context?\nThe geom_smooth() function, with method = lm, fits a linear model. What is the significance of the se argument in this function?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThe scatterplot encompasses 45 records, corresponding to half-hourly measurements over approximately 24 hours.\nIn the joined dataset, value.x and value.y represent the temperature and humidity values, respectively, after renaming to avoid duplication.\nThe se argument controls the display of confidence bounds around the regression line, which are shown when se is set to TRUE.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "10-data-viz.html#map-visualization",
    "href": "10-data-viz.html#map-visualization",
    "title": "10  Data Visualization",
    "section": "\n10.6 Map Visualization",
    "text": "10.6 Map Visualization\nIn our previous exploration, we learned how to employ the plot() function for basic map layouts. This section, however, elevates our cartographic journey by harnessing the versatility of the ggplot2 library for more intricate map visualizations.\nBefore diving into the examples, ensure you have the sf and ggplot2 libraries installed and loaded. Additionally, download the North Carolina and US States datasets for practical implementation.\nOur first endeavor is to construct a foundational map of North Carolina:\n\nlibrary(sf)\nlibrary(ggplot2)\n\n# Load North Carolina shapefile\nnc &lt;- sf::st_read(\"data/nc.shp\")\n\n# Basic map of North Carolina\nggplot(data = nc) +\n  geom_sf() +\n  xlab(\"Longitude\") + ylab(\"Latitude\") +\n  ggtitle(\"North Carolina\", subtitle = paste0(\"(\", length(unique(nc$NAME)), \" counties)\"))\n\n\n\n\n\n\n\nIn the code above, we first load the North Carolina shapefile as an sf() object and then assign the data to the ggplot() graph. The geom_sf function adds a geometry stored in a sf object. Other map components such as title and axis labels are added by means of a plus sign. Note that length(unique(nc$NAME)) returns the count of table rows, which corresponds to the number of geometries/counties. Geometry count and string “counties” are concatenated by function paste0().\nThe geometry element geom_sf provides a number of arguments to customize the appearance of vector features:\n\n# Map with custom colors\nggplot(data = nc) + \n  geom_sf(color = \"black\", fill = \"lightgreen\") +\n  xlab(\"Longitude\") + ylab(\"Latitude\") +\n  ggtitle(\"North Carolina\", subtitle = paste0(\"(\", length(unique(nc$NAME)), \" counties)\"))\n\n\n\n\n\n\n\nData attributes can be visually represented as well. In this example, the AREA variable influences the fill color:\n\n# Map with fill based on AREA\nggplot(data = nc) +\n  geom_sf(aes(fill = AREA)) +\n  scale_fill_viridis_c(option = \"plasma\", trans = \"sqrt\")  +\n  xlab(\"Longitude\") + ylab(\"Latitude\") +\n  ggtitle(\"North Carolina\", subtitle = paste0(\"(\", length(unique(nc$NAME)), \" counties)\"))\n\n\n\n\n\n\n\nThe function coord_sf() allows you to work with the coordinate system, which includes both the projection and extent of the map. By default, the map will use the coordinate system of the first layer or if the layer has no coordinate system, fall back on the geographic coordinate system WGS84. Using the argument crs, it is possible to override this setting, and project on the fly to any projection that has an EPSG code. For instance, we may change the coordinate system to EPSG 32618, which corresponds to WGS 84 / UTM zone 18N:\n\n# Map with changed coordinate system\nggplot(data = nc) +\n  geom_sf() +\n  coord_sf(crs = st_crs(32618)) + # Projecting to WGS 84 / UTM zone 18N  \n  xlab(\"Longitude\") + ylab(\"Latitude\") +\n  ggtitle(\"North Carolina\", subtitle = paste0(\"(\", length(unique(nc$NAME)), \" counties)\"))  \n\n\n\n\n\n\n\nThe extent of the map can also be set in coord_sf, in practice allowing to “zoom” in the area of interest, provided by limits on the x-axis (xlim), and on the y-axis (ylim). The limits are automatically expanded by a fraction to ensure that data and axes do not overlap; it can also be turned off to exactly match the limits provided with expand = FALSE:\n\nlibrary(\"ggspatial\")\nggplot(data = nc) +\n    geom_sf() +\n    coord_sf(xlim = c(-78.9, -75.5), ylim = c(34, 34.85), expand = FALSE) +\n    annotation_scale(location = \"br\", width_hint = 0.5) +\n    annotation_north_arrow(location = \"bl\", which_north = \"true\", \n        pad_x = unit(14.5, \"cm\"), pad_y = unit(0.8, \"cm\"),\n        style = north_arrow_fancy_orienteering) \n\n\n\n\n\n\n\n\nNote that scale bar and north arrow are available with package ggspatial.\n\nIn the following example, we will assign labels to vector features. The function geom_text() can be used to add a layer of text to a map using geographic coordinates. The North Carolina dataset contains county names as column (column: NAME). In order to define label positions, we take the centroids of the county polygons (function st_centroid()), derive x and y coordinates from centroids (function st_coordinates()), merge the new x and y columns with the columns of nc and assign the output to a new variable identifier nc_points:\n\nnc_points &lt;- cbind(nc, st_coordinates(st_centroid(nc$geometry)))\n\n\n\n\n\n\n\nExercise\n\n\n\nWe have used a standard syntax to create variabe nc_points. Convert the code to pipe operator syntax.\nBy the way, pipe operators are available with library magrittr, which is part of Tidyverse. So make sure to load it in your script.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nst_centroid(nc$geometry) %&gt;%\n  st_coordinates() %&gt;%\n  cbind(nc, .)\n\nNote that the reading direction of pipe syntax code is from left to right (more intuitive), whereas standard syntax (nested functions) is read from right to left.\n\n\n\nAfter deriving centroid coordinates from nc geometries, we call the new variable nc_points in function geom_text and map X and Y columns (centroid coordinates) onto visual variables x and y (position in graph) and also map column NAME onto visual variable label. Moreover, we can insert individual text annotations manually by means of function annotate():\n\nggplot(data = nc) +\ngeom_sf() +\ngeom_text(data= nc_points,aes(x=X, y=Y, label=NAME),\n    color = \"darkblue\", fontface = \"bold\", check_overlap = FALSE, size = 3) +\nannotate(geom = \"text\", x = -76.5, y = 34.3, label = \"Atlantic Ocean\", \n    fontface = \"italic\", color = \"grey22\", size = 5) +\ncoord_sf(xlim = c(-78.9, -75.5), ylim = c(34, 34.85), expand = FALSE) \n\n\n\n\n\n\n\nIn our final example, we combine previously introduced methods to craft a detailed map visualization. This example layers data sources, customizes feature appearances, and adds informative annotations:\n\nus_states &lt;- sf::st_read(\"data/us-states.shp\")\nus_states_points &lt;- st_centroid(us_states)\nus_states_points &lt;- cbind(us_states, st_coordinates(st_centroid(us_states$geometry)))\n\nggplot(data = nc) +\n  geom_sf(data = us_states, fill= \"antiquewhite1\") +\n  geom_sf(aes(fill = AREA)) +\n  geom_label(data= us_states_points,aes(x=X, y=Y, label=NAME),\n            color = \"black\", fontface = \"bold\", check_overlap = FALSE, size = 3, nudge_x = 0.5) +\n  annotation_scale(location = \"br\", width_hint = 0.5) +\n  annotation_north_arrow(location = \"bl\", which_north = \"true\", \n                         pad_x = unit(11, \"cm\"), pad_y = unit(0.8, \"cm\"),\n                         style = north_arrow_fancy_orienteering) + \n  scale_fill_viridis_c(trans = \"sqrt\", alpha = .4) +\n  coord_sf(xlim = c(-84.9, -70), ylim = c(24.5, 37), expand = FALSE) +\n  xlab(\"Longitude\") + ylab(\"Latitude\") +\n  ggtitle(\"US Southeast\", subtitle = \"(Detail: North Carolina)\") +\n  annotate(geom = \"text\", x = -76.5, y = 30.5, label = \"Atlantic Ocean\", \n           fontface = \"italic\", color = \"grey22\", size = 6) +\n  theme(panel.grid.major = element_line(color = gray(0.5), linetype = \"dashed\", \n       size = 0.1), panel.background = element_rect(fill = \"aliceblue\"))\n\n\n\n\n\n\n\nHere, geom_sf() adds layers of US state polygons. geom_label() offers an alternative to geom_text() for feature labeling, with nudge_x providing horizontal label offset. The map is saved in both PDF for high-quality prints and PNG for web use.\n\nggsave(\"data/map.pdf\")\nggsave(\"data/map_web.png\", width = 10, height = 10, dpi = \"screen\")\n\n\n\n\n\n\n\nTip\n\n\n\nRemember to consult the ggplot2 Cheatsheet for an overview of key ggplot2 operations.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "10-data-viz.html#interactive-maps",
    "href": "10-data-viz.html#interactive-maps",
    "title": "10  Data Visualization",
    "section": "\n10.7 Interactive Maps",
    "text": "10.7 Interactive Maps\nThe Leaflet library for R makes it easy to create interactive web maps. Leaflet is one of the most popular open-source JavaScript libraries used by a number of websites such as Airport Heathrow, Flickr or OpenStreetMap.\nStart by initializing a Leaflet map widget and augment it with base layers and interactive markers:\n\nlibrary(leaflet)\nm &lt;- leaflet() %&gt;%\n    addTiles() %&gt;%\n    addMarkers(lng=174.768, lat=-36.852, popup=\"The birthplace of R\")\n\nThe pipe operator (%&gt;%) conveniently adds layers, thanks to Leaflet’s function design.\n\n\n\n\n\n\nTip\n\n\n\nThe function addTiles() per default adds OpenStreetMap map tiles. You may use the function addProviderTiles() to add other map tiles. Leaflet supports a large number of basemap layers.\n\n\nThe same pipe-syntax can be used to add Markers and HTML Labels or Popups. In the following example, an HTML Popup locates a restaurant:\n\nlibrary(leaflet)\n\ncontent &lt;- paste(sep = \"&lt;br/&gt;\",\n                 \"&lt;b&gt;&lt;a href='https://www.techno-z.at/standort-und-service/gastronomie/'&gt;Bistro im Techno_Z&lt;/a&gt;&lt;/b&gt;\",\n                 \"Schillerstrasse 30\",\n                 \"5020 Salzburg\",\n                 \"This is where I had lunch today!\"\n)\n\nleaflet() %&gt;%\n  setView(lng = 13.040030, lat = 47.823112, zoom = 18) %&gt;%\n  addProviderTiles(\"OpenStreetMap.Mapnik\") %&gt;%\n  addPopups(13.040030, 47.823112, content,\n            options = popupOptions(closeButton = TRUE))\n\n\n\n\n\nMoreover, Leaflet offers numerous methods and functions for manipulating the map widget and integrating lines and shapes, GeoJSON and Raster Images. To get more information on creating interactive maps with R and Leaflet, turn to the Documentation.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "11-quarto.html",
    "href": "11-quarto.html",
    "title": "\n11  Quarto\n",
    "section": "",
    "text": "11.1 Set up your work environment\nAccording to the Quarto Website, Quarto is an open-source scientific and technical publishing system, and it is already available on your machines because you installed it together with RStudio.\nQuarto is part of a set of tools designed to enhance the reproducibility of your work. Other tools and platforms such as GitHub, Jupyter, Docker, ArXiv, and bioRxiv can facilitate reproducibility in various ways. In this module, we won’t explore the paradigm of reproducible research in detail. Instead, our focus will be on how to use Quarto to make your analyses and reports more appealing, interactive, and efficient.\nIn this lesson, we will weave together code and text in professionally rendered Quarto documents and use GitHub to safely store, share, and administer our results.\nBefore creating your first Quarto document, we need to set up the GitHub environment. Originally founded as a platform for software developers, GitHub’s architecture is designed to manage changes made during software development. This architecture is also beneficial for version control of documents or any information collection.\nVersion control is especially important when working in teams, as it helps synchronize efforts among project participants. However, GitHub is also a reliable and open online platform for individual work, providing change tracking, documentation, and sharing features.\nTo set up your personal GitHub environment, follow these steps:",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "11-quarto.html#set-up-your-work-environment",
    "href": "11-quarto.html#set-up-your-work-environment",
    "title": "\n11  Quarto\n",
    "section": "",
    "text": "Review the Hello-World Section in GitHub’s Quickstart Documentation. Initially, reading it is sufficient—no need to complete the tutorial yet.\n\nCreate a GitHub account.\n\nDownload and install Git. Git is a distributed VCS (version control system) that mirrors the codebase and its full history on every computer. GitHub is a web-based interface that integrates seamlessly with Git. For a clear explanation of Git’s core concepts, watch this video.\nIn RStudio (under Tools &gt; Global Options &gt; Git / SVN), check “enable version control” and set the path to git.exe (e.g., C:/Program Files/Git/bin/git.exe). Restart RStudio afterward.\n\nCreate a repository on GitHub. In the tutorial, skip the section ‘Commit your first changes’.\nBy default, your repository will have one branch named main. Create an additional branch called dev off the main. Follow the instructions in the Hello-World Tutorial for guidance.\n\n\n\n\n\n\n\nTip\n\n\n\nFor technical issues, please consult the discussion forum.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "11-quarto.html#create-a-local-clone",
    "href": "11-quarto.html#create-a-local-clone",
    "title": "\n11  Quarto\n",
    "section": "\n11.2 Create a local clone",
    "text": "11.2 Create a local clone\nTo work on your repository locally, you will need to create a local clone of your online GitHub repository. Here’s how:\nIn RStudio, go to (File &gt; New Project &gt; Version Control &gt; Git).\nEnter the URL of your online repository (find this URL in your GitHub repository) and select a local directory for the clone. Then click “Create Project”:\n\n\n\n\n\nFigure 11.1: Clone GitHub Repository\n\n\nOnce you have cloned the online repository, the file contents of the repository as well as a new tab called “Git” appears in RStudio:\n\n\n\n\n\nFigure 11.2: New features in RStudio\n\n\nBy default, the repository includes three files:\n\n\n.gitignore: Specifies intentionally untracked files to ignore.\n\nRStudio Project File (.Rproj): Contains metadata for the RStudio project.\n\nReadMe File (.md): A markdown file with information about the repository.\n\nThe gitignore and .Rproj files are created during project initialization and are not yet in the online repository. Modifications appear in the “Git” tab:\n\n\n\n\n\nFigure 11.3: Changes in Git tab\n\n\nBefore making further changes, switch to the dev branch:\n\n\n\n\n\nFigure 11.4: Switch branch\n\n\nAt this point, the dev branch mirrors the main branch.\n\n\n\n\n\n\nTip\n\n\n\nIt is highly recommended to work in progress on a separate developer branch, like dev, and keep the main branch for stable versions. You can later merge changes from dev to main through a pull request (see Opening a Pull Request).",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "11-quarto.html#creating-your-first-quarto-document",
    "href": "11-quarto.html#creating-your-first-quarto-document",
    "title": "\n11  Quarto\n",
    "section": "\n11.3 Creating Your First Quarto Document",
    "text": "11.3 Creating Your First Quarto Document\nNow that the environment is set up, let’s create our first Quarto document.\nIn RStudio: Navigate to (File &gt; New File &gt; Quarto Document). Enter a title for your document, accept the default settings, click “OK”, and save the file. You’ll receive a sample Quarto file with the extension .qmd.\nBy default, Quarto is in Visual Editing mode, which provides a WYSIWYM-style editing interface for Quarto documents. Switch to the Source Editor:\n\n\n\n\n\nFigure 11.5: Switch mode in RStudio\n\n\n\nWhile the Visual Editor interface is more intuitive, the source editor promotes a deeper understanding of underlying structures. Moreover, errors can be spotted and debugged more easily in Source Editor mode.\n\nQuarto documents include three core components. Metadata, text and code:\n\n\n\n\n\nFigure 11.6: Quarto sample file\n\n\nThe metadata, written in YAML syntax, defines document properties like title, output format, and creation date.\n\n\n\n\n\n\nExercise\n\n\n\nExplore YAML syntax and document properties here.\nInsert a suitable parameter in the metadata section of your Quarto document to include today’s date in the document header. Push the button “Render” in RStudio to generate the HTML output.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n---\ntitle: \"test\"\nformat: html\neditor: visual\ndate: today\n---\n\n\n\n\nSimilarly, you may change the output format under YAML parameter format. Quarto supports output formats like HTML, PDF, MS Word, ePub, Jupyter and many more (see all Quarto Formats).\n\n\n\n\n\n\nTip\n\n\n\nAlternatively, YAML-configurations may be specified on a project level (separate file named _quarto.yml) or on a code chunk level (see YAML Locations).\n\n\nIn your Quarto document you will find R inline code blocks that start and end with three backticks. The parameter r in curly brackets identifies the code as R code. Other languages in Quarto are Python, Julia and Observable JS.\nTo customize outputs of your code, you may specify execution options. For instance, the second code block in your Quarto document is not displayed in the HTML output due to execution option #| echo: false. Moreover, by setting execution option #| eval: false, code can be displayed in the HTML without being executed. Other code execution options can be found here.\n\n\n\n\n\n\nExercise\n\n\n\nInsert the following code into your Quarto document, add some textual interpretation of results and render as HTML:\n\nlibrary(ggplot2)\nggplot(data=cars, aes(x=speed, y=dist)) +\n  geom_point() +\n  geom_smooth()\n\n\n\nThis simple exercise illustrates the fundamental benefit of Quarto. It facilitates weaving together narrative text and code into data reports and documents that can be exported in various formats.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "11-quarto.html#synchronizing-with-github",
    "href": "11-quarto.html#synchronizing-with-github",
    "title": "\n11  Quarto\n",
    "section": "\n11.4 Synchronizing with GitHub",
    "text": "11.4 Synchronizing with GitHub\nRegular synchronization of your local changes with the online repository is a key practice in version control. Start by pulling any updates from the repository.\nIn the RStudio Git tab, click the “Pull” button (see Figure 11.7). A notification should indicate whether any new changes are available (e.g., Already up to date).\n\n\n\n\n\nFigure 11.7: Make Pull\n\n\nEven if you’re working on your own, it’s a good idea to routinely start the sync process with a “Pull”.\nNext, commit your changes. Think of committing as taking a snapshot of your progress, accompanied by a descriptive message.\nFirst, save all documents in RStudio. Then, hit the “Commit” button in the Git tab. The commit window will display a list of modified files. Green highlights indicate new content; red highlights show deleted content.\nCheck the boxes next to each file to include them in the commit. Alternatively, run git add -A in the terminal to add all files at once (see this list of popular Git commands). After selecting files, enter a meaningful commit message and click “Commit”.\nSee Figure 11.8.\n\n\n\n\n\nFigure 11.8: Make Commit\n\n\nFinally, push your committed changes to the online repository:\n\n\n\n\n\nFigure 11.9: Make Push\n\n\nYour online repository on GitHub should now be updated (switch to dev branch in your repository) (see Figure 11.10).\n\n\n\n\n\nFigure 11.10: Commit with message ‘test’ was pushed to the dev branch a minute ago\n\n\nIn our example the dev branch is two commits ahead of the main branch. You may open a pull request to merge changes from dev to main branch.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "11-quarto.html#basic-markdown-syntax",
    "href": "11-quarto.html#basic-markdown-syntax",
    "title": "\n11  Quarto\n",
    "section": "\n11.5 Basic Markdown Syntax",
    "text": "11.5 Basic Markdown Syntax\nIn Quarto text is formatted by means of the Markdown syntax. Commonly used markers are…\nBold: Double asterisks **Text** turn text bold.\nItalicize: Single asterisks *Text* create italicized text.\nHeadings: Use hash signs # for headings. The number of hashes denotes the heading level:\n# Heading level 1\n\n## Heading level 2\n\n### Heading level 3\nTables are created by using the symbols | and -. Recall the numeric operators table from the first lesson. Figure 11.11 shows the Markdown syntax used for that table:\n\n\n\n\n\nFigure 11.11: How Tables are made in Markdown\n\n\nTo create an ordered list, use numbers followed by a period. The first item should start with the number 1:\nCode - Ordered List:\n1. item 1\n4. item 2\n3. Item 3\n    + Item 3a\n    + Item 3b\nWill result in:\n\nItem 1\nItem 2\nItem 3\n\nItem 3a\nItem 3b\n\n\n\nTo create an unordered list, use *, -, or +:\nCode - Unordered List:\n* item 1\n* item 2\n  * Item 3.1\n  - Item 3.2\nWhich will result in:\n\nItem 1\nItem 2\n\nItem 2a\nItem 2b\n\n\n\nHyperlinks are created with the format [Text](URL), for example, [GitHub](https://github.com/){target=\"_blank\"} becomes GitHub. The target=\"_blank\" parameter opens the link in a new tab, which is a good practice when linking to external websites.\nBlockquotes are indicated by &gt; and can be nested:\n&gt;\"Everything is related to everything else, but near things are more related than distant things\".\n&gt;\n&gt;&gt;The phenomenon external to an area of interest affects what goes on inside.\nWill result in:\n\nThe first law of geography is: “Everything is related to everything else, but near things are more related than distant things”\n\nThe phenomenon external to an area of interest affects what goes on inside.\n\n\nMeanwhile, you know several characters that have a special meaning in Markdown syntax (like # or &gt;). If you want these characters verbatim, you have to escape them. The way to escape a special character is to add a backslash before. For instance, \\# will not translate into a heading, but will return #.\nRMarkdown supports a large number of mathematical notations using dollar signs $:\nMath. notation example 1:\n$x = y$\nResult looks like:\nx = y\nMath. notation example 2:\n$\\frac{\\partial f}{\\partial x}$\nResult looks like:\n\\frac{\\partial f}{\\partial x}\n\n\n\n\n\n\nTip\n\n\n\nSee “Mathematics in R Markdown” as well as Markdown Basics for more.\n\n\n\n11.5.1 References in Quarto\nQuarto facilitates an efficient method for inserting citations and building a bibliography. References are organized in a .bib file.\nTo begin, create a new document in a text editor, such as Windows Editor, and save it with a .bib extension (e.g., references.bib) in your RStudio project folder.\n\n\nEnable BibTeX Export: Modify your settings in Google Scholar to enable BibTeX export (see Figure 11.12).\n\n\n\n\n\n\nFigure 11.12: Enable BibTeX in Firefox 106.0.1\n\n\n\nBrowser versions may vary. For assistance, refer to the discussion forum if needed.\n\n\n\nExport BibTeX Entries: After enabling BibTeX export, a new link “Import into BibTeX” will appear in Google Scholar (see Figure 11.13).\n\n\n\n\n\n\nFigure 11.13: BibTeX Link in Firefox 106.0.1\n\n\nClick the link and copy the BibTeX code into your .bib file.\n\n\nIntegrate References in the Quarto document: Specify the location of your .bib file in the YAML metadata of your Quarto document (bibliography: &lt;.bib file&gt;). Insert @ followed by the BibTeX key to add citations (see Figure 11.14).\n\n\n\n\n\n\nFigure 11.14: Integrate BibTeX reference in Quarto document\n\n\n\n\nCompile the Document: Render the Quarto document as HTML. Quarto processes both indirect (without square brackets) and direct citations (with square brackets) and includes a bibliography (see Figure 11.15).\n\n\n\n\n\n\nFigure 11.15: Quarto with reference\n\n\nFor a practical demonstration, download and explore this Quarto reference example. Unzip the folder and open the .Rproj file in RStudio.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "11-quarto.html#speed-up-your-workflows",
    "href": "11-quarto.html#speed-up-your-workflows",
    "title": "\n11  Quarto\n",
    "section": "\n11.6 Speed up your workflows",
    "text": "11.6 Speed up your workflows\nQuarto significantly enhances the efficiency of repetitive workflows. For instance, consider a scenario where a client requires daily updates on specific spatial economic indicators. Instead of manually generating a new report each day, Quarto can automate this process, creating data reports with charts that update automatically upon compilation. This approach can save substantial time and effort.\nReal-time data retrieval is possible through Alpha Vantage, which provides financial market data via the Alpha Vantage Rest API. The R library alphavantager facilitates API access within R. The use of alphavantager enables the extraction of various types of financial data, including real-time stock prices, FX rates, and technical indicators, directly into R. This allows for efficient data processing and visualization, making it a good tool for finance-related reports and analyses in Quarto.\n\n\n\n\n\n\nExercise\n\n\n\nExplore a practical example by downloading this draft finance data report. Unzip the folder and open the .Rproj file in RStudio.\nThe project includes:\n\nA .bib file with a BibTeX reference.\nA .csv file in the data folder, listing over 400 country names, national currencies, and currency codes.\nA .qmd file with inline R code that renders real-time currency exchange rates in a map.\n\nReview the .qmd file thoroughly before compiling an HTML output. Note that it includes an interactive Leaflet map, making HTML the only supported output format.\nTry enhancing the report with an additional spatial indicator, such as a map displaying exchange rates from national currencies to the Euro.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "11-quarto.html#self-study",
    "href": "11-quarto.html#self-study",
    "title": "\n11  Quarto\n",
    "section": "\n11.7 Self-study",
    "text": "11.7 Self-study\nThe vast functionalities of Quarto extend beyond the scope of a single lesson. To fully exploit its capabilities, refer to the comprehensive Quarto Guide.\nThis guide covers additional topics such as the integration of figures or cross references, computations in various languages such as R, Julia and Observable and formats such as quarto projects, presentations, dashboards, websites, books or manuscripts.\n\n\n\n\n\n\nTip\n\n\n\nTo find inspiration for your own projects you may consult the Quarto Gallery that provides a variety of Quarto best practice examples.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Preface",
    "section": "",
    "text": "This web-book is a text book with exercises that together form the learning materials for “Automated Data Processing with R”, an elective module of the UNIGIS distance learning program in Geoinformatics at the University of Salzburg.\n  \nThe web-book is published under an open licence. We welcome everybody to explore the contents and to work through the exercises. A certificate of completion and online support by a tutor can exclusively be claimed by those who are signed up for this UNIGIS module at the University of Salzburg.\n\nFor more information, please get in contact with the UNIGIS Office.",
    "crumbs": [
      "Preface"
    ]
  }
]